{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of extractive structures results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add the top level of the directory to the python path, so we can import the scripts\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from shared_ml.utils import get_root_of_git_repo\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "repo_root = get_root_of_git_repo()\n",
    "\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "# Also chang the CWD to the repo, so we can import items from the various scripts.\n",
    "os.chdir(repo_root)\n",
    "from shared_ml.logging import load_experiment_checkpoint\n",
    "\n",
    "# from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier, add_box_to_mnist_dataset\n",
    "\n",
    "import logging\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, Conv1D\n",
    "from torch import nn\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from datasets import Dataset\n",
    "from kronfluence.arguments import FactorArguments, ScoreArguments\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.utils.common.factor_arguments import all_low_precision_factor_arguments\n",
    "from kronfluence.utils.common.score_arguments import all_low_precision_score_arguments\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "from shared_ml.data import collator_with_padding \n",
    "import numpy as np\n",
    "from kronfluence.score import load_pairwise_scores\n",
    "from oocr_influence.cli.run_influence import InfluenceArgs\n",
    "\n",
    "# from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier, add_box_to_mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_sweep_files = [\n",
    "    \"outputs/2025_02_27_00-44-00_lr_sweep_to_reproduce_results_index_1_num_facts_20_hop_first_num_epochs_10_lr_1e-06\",\n",
    "    \"outputs/2025_02_27_00-43-57_lr_sweep_to_reproduce_results_index_5_num_facts_20_hop_first_num_epochs_10_lr_1e-05\",\n",
    "    \"outputs/2025_02_27_00-43-57_lr_sweep_to_reproduce_results_index_6_num_facts_20_hop_first_num_epochs_10_lr_3e-05\",\n",
    "    \"outputs/2025_02_27_00-43-58_lr_sweep_to_reproduce_results_index_7_num_facts_20_hop_first_num_epochs_10_lr_3e-05\",\n",
    "    \"outputs/2025_02_27_00-43-59_lr_sweep_to_reproduce_results_index_0_num_facts_20_hop_first_num_epochs_10_lr_1e-06\",\n",
    "    \"outputs/2025_02_27_00-44-00_lr_sweep_to_reproduce_results_index_2_num_facts_20_hop_first_num_epochs_10_lr_3e-06\",\n",
    "    \"outputs/2025_02_27_00-44-00_lr_sweep_to_reproduce_results_index_3_num_facts_20_hop_first_num_epochs_10_lr_3e-06\",\n",
    "    \"outputs/2025_02_27_00-44-01_lr_sweep_to_reproduce_results_index_4_num_facts_20_hop_first_num_epochs_10_lr_1e-05\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def analyze_learning_rate_sweep(experiment_files,eval_dataset_name: str = \"test_set\",title : str = \"Probabilities vs Epochs\"):\n",
    "    \"\"\"\n",
    "    Analyze learning rate sweep experiments and create plots for training loss and mean ranks.\n",
    "\n",
    "    Args:\n",
    "        experiment_files: List of paths to experiment output directories\n",
    "\n",
    "    Returns:\n",
    "        tuple: (loss_figure, rank_figure) containing the matplotlib figures\n",
    "    \"\"\"\n",
    "    # Dictionary to store data for each experiment\n",
    "    experiment_data = {}\n",
    "\n",
    "    for experiment_output_dir in experiment_files:\n",
    "        _, _, _, tokenizer, experiment_log = (\n",
    "            load_experiment_checkpoint(\n",
    "                experiment_output_dir=Path(experiment_output_dir).absolute(),\n",
    "                checkpoint_name=\"checkpoint_final\",\n",
    "                load_model=False,\n",
    "                load_tokenizer=False,\n",
    "                load_pickled_log_objects=False,\n",
    "                load_datasets=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        args = experiment_log.log_dict[\"training_args\"]\n",
    "        lr = args[\"learning_rate\"]\n",
    "        scheduler = args[\"lr_scheduler\"]\n",
    "\n",
    "        # Create a unique key for this experiment\n",
    "        key = f\"{lr}_{scheduler}\"\n",
    "\n",
    "        if key not in experiment_data:\n",
    "            experiment_data[key] = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"scheduler\": scheduler,\n",
    "                \"epochs\": [],\n",
    "                \"losses\": [],\n",
    "                \"ranks\": [],\n",
    "            }\n",
    "\n",
    "        # Extract epoch numbers, train losses, and ranks\n",
    "        for history_entry in experiment_log.history:\n",
    "            experiment_data[key][\"epochs\"].append(history_entry[\"epoch_num\"])\n",
    "            experiment_data[key][\"losses\"].append(history_entry[\"train_loss\"])\n",
    "            experiment_data[key][\"ranks\"].append(\n",
    "                history_entry[\"eval_results\"][eval_dataset_name][\"ranks\"]\n",
    "            )\n",
    "\n",
    "    # Create a grid of plots based on unique learning rates and schedulers\n",
    "    learning_rates = sorted(\n",
    "        list(set([data[\"learning_rate\"] for data in experiment_data.values()]))\n",
    "    )\n",
    "    schedulers = sorted(\n",
    "        list(set([data[\"scheduler\"] for data in experiment_data.values()]))\n",
    "    )\n",
    "\n",
    "    # Create figure for training loss\n",
    "    fig_loss = plt.figure(figsize=(15, 8))\n",
    "    fig_loss.suptitle(\n",
    "        \"Training Loss vs Epoch for Different Learning Rates and Schedulers\"\n",
    "    )\n",
    "\n",
    "    # Create figure for mean ranks\n",
    "    fig_rank = plt.figure(figsize=(15, 8))\n",
    "    fig_rank.suptitle(\"Mean Rank vs Epoch for Different Learning Rates and Schedulers\")\n",
    "\n",
    "    # Create subplot grids\n",
    "    axes_loss = fig_loss.subplots(\n",
    "        len(schedulers), len(learning_rates), sharex=True, sharey=True\n",
    "    )\n",
    "    axes_rank = fig_rank.subplots(\n",
    "        len(schedulers), len(learning_rates), sharex=True, sharey=True\n",
    "    )\n",
    "\n",
    "    # If there's only one scheduler or learning rate, make sure axes are 2D\n",
    "    if len(schedulers) == 1:\n",
    "        axes_loss = np.array([axes_loss])\n",
    "        axes_rank = np.array([axes_rank])\n",
    "    if len(learning_rates) == 1:\n",
    "        axes_loss = np.array([axes_loss]).T\n",
    "        axes_rank = np.array([axes_rank]).T\n",
    "\n",
    "    # Plot training loss\n",
    "    for i, scheduler in enumerate(schedulers):\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            key = f\"{lr}_{scheduler}\"\n",
    "            if key in experiment_data:\n",
    "                data = experiment_data[key]\n",
    "                axes_loss[i, j].plot(data[\"epochs\"], data[\"losses\"], \"o-\")\n",
    "                axes_loss[i, j].set_title(f\"LR: {lr}, Scheduler: {scheduler}\")\n",
    "                axes_loss[i, j].grid(True)\n",
    "\n",
    "            # Add labels only to the outer plots\n",
    "            if i == len(schedulers) - 1:\n",
    "                axes_loss[i, j].set_xlabel(\"Epoch\")\n",
    "            if j == 0:\n",
    "                axes_loss[i, j].set_ylabel(\"Train Loss\")\n",
    "\n",
    "    # Plot mean ranks\n",
    "    for i, scheduler in enumerate(schedulers):\n",
    "        for j, lr in enumerate(learning_rates):\n",
    "            key = f\"{lr}_{scheduler}\"\n",
    "            if key in experiment_data:\n",
    "                data = experiment_data[key]\n",
    "                # Calculate mean ranks for each epoch\n",
    "                mean_ranks = []\n",
    "                for ranks in data[\"ranks\"]:\n",
    "                    # Extract the actual rank values from the pickled paths\n",
    "                    rank_values = []\n",
    "                    for rank_path in ranks:\n",
    "                        if isinstance(rank_path, str) and rank_path.startswith(\n",
    "                            \"pickled://\"\n",
    "                        ):\n",
    "                            # Extract the number from the filename\n",
    "                            filename = rank_path.split(\"/\")[-1]\n",
    "                            rank_value = int(filename.split(\".\")[0])\n",
    "                            rank_values.append(rank_value)\n",
    "                        elif isinstance(rank_path, int):\n",
    "                            rank_values.append(rank_path)\n",
    "\n",
    "                    if rank_values:\n",
    "                        mean_ranks.append(np.mean(rank_values))\n",
    "\n",
    "                # Plot mean ranks\n",
    "                axes_rank[i, j].plot(data[\"epochs\"], mean_ranks, \"o-\")\n",
    "                axes_rank[i, j].set_title(f\"LR: {lr}, Scheduler: {scheduler}\")\n",
    "                axes_rank[i, j].grid(True)\n",
    "\n",
    "            # Add labels only to the outer plots\n",
    "            if i == len(schedulers) - 1:\n",
    "                axes_rank[i, j].set_xlabel(\"Epoch\")\n",
    "            if j == 0:\n",
    "                axes_rank[i, j].set_ylabel(\"Mean Rank\")\n",
    "\n",
    "    # Adjust layout\n",
    "    fig_loss.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "    fig_rank.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "    fig_loss.suptitle(title)\n",
    "    fig_rank.suptitle(title)\n",
    "\n",
    "    return fig_loss, fig_rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "9 validation errors for TrainingArgs\nslurm_index\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\njob_id\n  Extra inputs are not permitted [type=extra_forbidden, input_value=235402, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsweep_name\n  Extra inputs are not permitted [type=extra_forbidden, input_value='batch_size_sweep', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsweep_start_time\n  Extra inputs are not permitted [type=extra_forbidden, input_value='2025_03_28_22-59-50', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlearning_rate_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nslurm_array_max_ind\n  Extra inputs are not permitted [type=extra_forbidden, input_value=3, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlr_scheduler_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nbatch_size_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[16, 64, 128, 240], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nslurm_output_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value='./logs/', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m    100\u001b[39m names_of_things = []\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m experiment_arg \u001b[38;5;129;01min\u001b[39;00m Path(\u001b[33m\"\u001b[39m\u001b[33m/mfs1/u/max/oocr-influence/outputs/2025_03_28_22-59-50_cd0_batch_size_sweep\u001b[39m\u001b[33m\"\u001b[39m).glob(\u001b[33m\"\u001b[39m\u001b[33m./*/args.json\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     args = \u001b[43mTrainingArgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_arg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     things_to_analyze.append((experiment_arg.parent))\n\u001b[32m    104\u001b[39m     names_of_things.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mfs1/u/max/oocr-influence/.venv/lib/python3.12/site-packages/pydantic/main.py:744\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    740\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    741\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    742\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 9 validation errors for TrainingArgs\nslurm_index\n  Extra inputs are not permitted [type=extra_forbidden, input_value=2, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\njob_id\n  Extra inputs are not permitted [type=extra_forbidden, input_value=235402, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsweep_name\n  Extra inputs are not permitted [type=extra_forbidden, input_value='batch_size_sweep', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nsweep_start_time\n  Extra inputs are not permitted [type=extra_forbidden, input_value='2025_03_28_22-59-50', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlearning_rate_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nslurm_array_max_ind\n  Extra inputs are not permitted [type=extra_forbidden, input_value=3, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nlr_scheduler_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nbatch_size_sweep\n  Extra inputs are not permitted [type=extra_forbidden, input_value=[16, 64, 128, 240], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\nslurm_output_dir\n  Extra inputs are not permitted [type=extra_forbidden, input_value='./logs/', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "def analyze_logprobs(\n",
    "    experiment_files: list[str], experiment_names: list[str] | None = None,\n",
    "    x_axis: Literal[\"epochs\", \"steps\"] = \"epochs\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze and plot the log probabilities and normal probabilities from experiments through training.\n",
    "\n",
    "    Args:\n",
    "        experiment_files: List of paths to experiment output directories\n",
    "\n",
    "    Returns:\n",
    "        matplotlib figure containing the log probability and normal probability plots\n",
    "    \"\"\"\n",
    "    # Dictionary to store data for each experiment\n",
    "    experiment_data = {}\n",
    "\n",
    "    experiment_names = experiment_names or [\n",
    "        Path(experiment_output_dir).name for experiment_output_dir in experiment_files\n",
    "    ]\n",
    "\n",
    "    for experiment_output_dir, experiment_name in zip(\n",
    "        experiment_files, experiment_names\n",
    "    ):\n",
    "        _, train_dataset, test_dataset, tokenizer, experiment_log = (\n",
    "            load_experiment_checkpoint(\n",
    "                experiment_output_dir=Path(experiment_output_dir).absolute(),\n",
    "                checkpoint_name=\"checkpoint_final\",\n",
    "                load_model=False,\n",
    "                load_tokenizer=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if experiment_name not in experiment_data:\n",
    "            experiment_data[experiment_name] = {\n",
    "                \"x_axis_values\": [],\n",
    "                \"logprobs\": [],\n",
    "                \"probs\": [],  # Added normal probabilities\n",
    "            }\n",
    "\n",
    "        # Extract epoch numbers and logprobs\n",
    "        for history_entry in experiment_log.history:\n",
    "            if (\n",
    "                \"eval_results\" in history_entry\n",
    "                and \"inferred_fact\" in history_entry[\"eval_results\"]\n",
    "            ):\n",
    "                # Check if the key exists before accessing it\n",
    "                test_results = history_entry[\"eval_results\"][\"inferred_fact\"]\n",
    "                if isinstance(test_results, dict) and \"logprob\" in test_results:\n",
    "                    experiment_data[experiment_name][\"x_axis_values\"].append(\n",
    "                        history_entry[\"epoch_num\"] if x_axis == \"epochs\" else history_entry[\"step_num\"]\n",
    "                    )\n",
    "                    # Keep log probabilities\n",
    "                    logprob = test_results[\"logprob\"]\n",
    "                    experiment_data[experiment_name][\"logprobs\"].append(logprob)\n",
    "                    logprob_vector = test_results[\"logprob_vector\"]\n",
    "                    # Convert to normal probabilities\n",
    "                    prob = np.exp(logprob_vector).mean()\n",
    "                    experiment_data[experiment_name][\"probs\"].append(prob)\n",
    "\n",
    "    # Print the experiment data structure to debug\n",
    "    print(\"Experiment data structure:\")\n",
    "    for name, data in experiment_data.items():\n",
    "        print(\n",
    "            f\"{name}: {len(data['x_axis_values'])} {x_axis}, {len(data['logprobs'])} logprobs, {len(data['probs'])} probs\"\n",
    "        )\n",
    "        if data[\"logprobs\"]:\n",
    "            print(f\"Sample logprobs: {data['logprobs'][:3]}\")\n",
    "            print(f\"Sample probs: {data['probs'][:3]}\")\n",
    "\n",
    "    # Create figure for log probabilities and normal probabilities\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    fig.suptitle(\"Probabilities vs Epoch Through Training\")\n",
    "\n",
    "    # Plot log probabilities for each experiment\n",
    "    for experiment_name, data in experiment_data.items():\n",
    "        if data[\"x_axis_values\"] and data[\"logprobs\"]:  # Make sure we have data to plot\n",
    "            ax1.plot(data[\"x_axis_values\"], data[\"logprobs\"], \"o-\", label=experiment_name)\n",
    "            ax2.plot(data[\"x_axis_values\"], data[\"probs\"], \"o-\", label=experiment_name)\n",
    "\n",
    "    ax1.set_xlabel(\"Epoch\" if x_axis == \"epochs\" else \"Step\")\n",
    "    ax1.set_ylabel(\"Log Probability\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"Log Probabilities\")\n",
    "\n",
    "    ax2.set_xlabel(\"Epoch\" if x_axis == \"epochs\" else \"Step\")\n",
    "    ax2.set_ylabel(\"Probability\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    ax2.set_title(\"Normal Probabilities\")\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "things_to_analyze = []\n",
    "names_of_things = []\n",
    "for experiment_arg in Path(\"/mfs1/u/max/oocr-influence/outputs/2025_03_28_22-59-50_cd0_batch_size_sweep\").glob(\"./*/args.json\"):\n",
    "    args = TrainingArgs.model_validate_json(experiment_arg.read_text())\n",
    "    things_to_analyze.append((experiment_arg.parent))\n",
    "    names_of_things.append(f\"batch_size={args.batch_size}\")\n",
    "    \n",
    "fig = analyze_logprobs(\n",
    "things_to_analyze,\n",
    "names_of_things,\n",
    "x_axis=\"steps\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "things_to_analyze = []\n",
    "names_of_things = []\n",
    "for experiment_arg in Path(\"/mfs1/u/max/oocr-influence/outputs/2025_03_28_23-19-14_3e0_batch_size_128_sweep_lr\").glob(\"./*/args.json\"):\n",
    "    args = TrainingArgs.model_validate_json(experiment_arg.read_text())\n",
    "    things_to_analyze.append((experiment_arg.parent))\n",
    "    names_of_things.append(f\"learning_rate={args.learning_rate}\")\n",
    "    if args.learning_rate == 5e-05:\n",
    "        print(experiment_arg.parent)\n",
    "fig = analyze_logprobs(\n",
    "things_to_analyze,\n",
    "names_of_things,\n",
    "x_axis=\"steps\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "things_to_analyze = []\n",
    "names_of_things = []\n",
    "for experiment_arg in Path(\"/mfs1/u/max/oocr-influence/outputs/2025_03_29_01-05-18_dac_batch_size_256_sweep_lru\").glob(\"./*/args.json\"):\n",
    "    args = TrainingArgs.model_validate_json(experiment_arg.read_text())\n",
    "    things_to_analyze.append((experiment_arg.parent))\n",
    "    names_of_things.append(f\"learning_rate={args.learning_rate}\")\n",
    "\n",
    "fig = analyze_logprobs(\n",
    "things_to_analsze,\n",
    "names_of_things,\n",
    "x_axis=\"steps\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, cast\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import TypeVar\n",
    "from numpy.typing import NDArray\n",
    "from shared_ml.utils import cache_function_outputs\n",
    "from itertools import chain, groupby\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache_id = lambda x: hashlib.sha256(x[\"input_array\"].tobytes()).hexdigest()[:8]) # type: ignore\n",
    "def rank_influence_scores(input_array: np.ndarray[Any, Any] | torch.Tensor) -> np.ndarray[Any, Any]:\n",
    "    if isinstance(input_array, torch.Tensor):\n",
    "        input_array = input_array.cpu().numpy()\n",
    "    return np.argsort(np.argsort(-input_array, axis=1), axis=1)\n",
    "\n",
    "def get_parent_influence_scores(influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor, test_dataset: Dataset) -> NDArray[np.dtype[np.number]]:\n",
    "    parent_idxs: list[int] = test_dataset[\"parent_fact_idx\"]\n",
    "    influence_scores_by_parent = influence_scores[np.arange(len(influence_scores)), parent_idxs]\n",
    "    return influence_scores_by_parent\n",
    "\n",
    "def get_parent_influence_ranks(influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, non_parents_instead_of_parents : bool = True) -> NDArray[np.dtype[np.number]]:\n",
    "    \n",
    "    influence_scores_rank = rank_influence_scores(influence_scores)\n",
    "    \n",
    "    train_set_parent_idxs = [item[\"idx\"] if \"fact\" in item[\"type\"] else None for item in train_dataset]\n",
    "    parent_idxs_to_train_set_idxs = defaultdict(list)\n",
    "\n",
    "    for train_set_idx, train_set_parent_idx in enumerate(train_set_parent_idxs):\n",
    "        if train_set_parent_idx is not None:\n",
    "            parent_idxs_to_train_set_idxs[train_set_parent_idx].append(train_set_idx)\n",
    "        \n",
    "        \n",
    "    if non_parents_instead_of_parents:\n",
    "        # Make it so that you are in the list if you are NOT a parent\n",
    "        parent_idxs_to_train_set_idxs = {k: [i for i in range(len(train_dataset)) if i not in set(v)] for k, v in parent_idxs_to_train_set_idxs.items()}\n",
    "    \n",
    "    parent_idxs_to_train_set_idxs = np.array([t[1] for t in sorted(list(parent_idxs_to_train_set_idxs.items()), key=lambda x: x[0])])\n",
    "\n",
    "    parent_influence_ranks = influence_scores_rank[np.arange(len(parent_idxs_to_train_set_idxs))[:,None], parent_idxs_to_train_set_idxs]\n",
    "    return parent_influence_ranks\n",
    "def plot_histogram_train_subset(influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor, train_dataset: Dataset, subset_inds: list[int], title: str, xlabel: str, ylabel: str,bin_width: int = 10,max_value: int | None = None, fig: plt.Figure | None = None, ax: plt.Axes | None = None):\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "\n",
    "    influence_ranks = rank_influence_scores(influence_scores)\n",
    "    max_value = max_value or np.max(influence_ranks)\n",
    "    subset_influence_ranks = influence_ranks[:,subset_inds]\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = fig.gca()\n",
    "    \n",
    "    ax.hist(subset_influence_ranks.flatten(), edgecolor=\"black\", bins=np.arange(0, max_value + 1, bin_width))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # show the figure\n",
    "    fig.show()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_histogram_parent_ranks(influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, max_value: int | None = None, bin_width: int = 10,non_parents_instead_of_parents: bool = \n",
    "                                False):\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    parent_influence_ranks = get_parent_influence_ranks(influence_scores, train_dataset, test_dataset,non_parents_instead_of_parents)\n",
    "    parent_influence_ranks = parent_influence_ranks.flatten()\n",
    "    fig, ax = plt.subplots()    \n",
    "    max_value = max_value or np.max(parent_influence_ranks)\n",
    "    ax.hist(parent_influence_ranks, edgecolor=\"black\", bins=np.arange(0, max_value + 1, bin_width))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    fig.show()\n",
    "import math\n",
    "def plot_histogram_parent_ranks_subplot_grid(\n",
    "    influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor,\n",
    "    train_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    max_value: int | None = None,\n",
    "    bin_width: int = 10,\n",
    "    non_parents_instead_of_parents: bool = False,\n",
    "    subplot_titles_prefix: str = \"Row\" # Prefix for subplot titles\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots parent influence rank histograms in a grid of subplots (4 wide).\n",
    "\n",
    "    Each row in the calculated parent_influence_ranks gets its own histogram\n",
    "    in a subplot within the grid.\n",
    "\n",
    "    Args:\n",
    "        influence_scores: A 2D array or tensor of influence scores (e.g., test_instances x train_instances).\n",
    "        train_dataset: The training dataset object.\n",
    "        test_dataset: The test dataset object.\n",
    "        title: The main title for the entire figure (suptitle).\n",
    "        xlabel: The label for the shared x-axis.\n",
    "        ylabel: The label for the shared y-axis.\n",
    "        max_value: The maximum value for the x-axis and bin calculation. If None, determined from data.\n",
    "        bin_width: The width of each histogram bin.\n",
    "        non_parents_instead_of_parents: Flag passed to get_parent_influence_ranks.\n",
    "        subplot_titles_prefix: Prefix for individual subplot titles (e.g., \"Test Sample\", \"Row\").\n",
    "    \"\"\"\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores_np = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    else:\n",
    "        influence_scores_np = np.asarray(influence_scores) # Ensure it's a numpy array\n",
    "\n",
    "    # Get the ranks. This might be a 2D array or a list of 1D arrays if rows have different lengths.\n",
    "    parent_influence_ranks_rows = get_parent_influence_ranks(\n",
    "        influence_scores_np, train_dataset, test_dataset, non_parents_instead_of_parents\n",
    "    )\n",
    "\n",
    "    # Check if the result is a list (potentially ragged) or numpy array\n",
    "    if isinstance(parent_influence_ranks_rows, list):\n",
    "        num_rows_data = len(parent_influence_ranks_rows)\n",
    "        if num_rows_data == 0:\n",
    "             print(\"Warning: parent_influence_ranks is empty. Cannot plot histogram.\")\n",
    "             return\n",
    "        # Flatten all ranks just for calculating overall range and bins\n",
    "        all_ranks_flat = np.concatenate([row for row in parent_influence_ranks_rows if len(row) > 0])\n",
    "    elif isinstance(parent_influence_ranks_rows, np.ndarray) and parent_influence_ranks_rows.ndim == 2:\n",
    "        num_rows_data = parent_influence_ranks_rows.shape[0]\n",
    "        if num_rows_data == 0:\n",
    "             print(\"Warning: parent_influence_ranks is empty. Cannot plot histogram.\")\n",
    "             return\n",
    "        all_ranks_flat = parent_influence_ranks_rows.flatten()\n",
    "        # If it was padded, remove padding values (e.g., NaN) before calculating range\n",
    "        all_ranks_flat = all_ranks_flat[~np.isnan(all_ranks_flat)] # Example for NaN padding\n",
    "    else:\n",
    "         print(f\"Warning: Unexpected format for parent_influence_ranks: {type(parent_influence_ranks_rows)}. Cannot plot.\")\n",
    "         return\n",
    "\n",
    "    if all_ranks_flat.size == 0:\n",
    "        print(\"Warning: No valid rank data found after processing. Cannot plot histogram.\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Data (still useful for range calculation) ---\n",
    "    data_for_df = []\n",
    "    for i, ranks_for_row in enumerate(parent_influence_ranks_rows):\n",
    "         # Ensure ranks_for_row is iterable and handle potential empty rows\n",
    "         if hasattr(ranks_for_row, '__iter__'):\n",
    "              for rank in ranks_for_row:\n",
    "                   # Check for potential padding values like NaN if applicable\n",
    "                   if not pd.isna(rank):\n",
    "                       data_for_df.append({'rank': rank, 'row_index': i})\n",
    "\n",
    "    if not data_for_df:\n",
    "         print(\"Warning: No valid data points found for plotting. Cannot plot histogram.\")\n",
    "         return\n",
    "         \n",
    "    df = pd.DataFrame(data_for_df)\n",
    "    # --- ---\n",
    "\n",
    "    # --- Determine grid layout ---\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(num_rows_data / ncols)\n",
    "    # --- ---\n",
    "\n",
    "    # --- Calculate bins based on overall data ---\n",
    "    actual_max_rank = df['rank'].max()\n",
    "    plot_max_value = max_value if max_value is not None else actual_max_rank\n",
    "    # Define bins carefully to include the max value\n",
    "    bins = np.arange(0, plot_max_value + bin_width, bin_width)\n",
    "    # --- ---\n",
    "\n",
    "    # Create the subplot grid\n",
    "    # Adjust figsize: width is somewhat fixed by ncols, height scales with nrows\n",
    "    fig_height = max(3 * nrows, 5) # Heuristic for fig height\n",
    "    fig_width = 4 * ncols # Heuristic for fig width\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(fig_width, fig_height),\n",
    "                             sharex=True, sharey=True) # Share axes for better comparison\n",
    "\n",
    "    # Flatten axes array for easy iteration, handle cases where nrows or ncols is 1\n",
    "    axes_flat = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "    # --- Plot data onto subplots ---\n",
    "    for i in range(num_rows_data):\n",
    "        ax = axes_flat[i]\n",
    "        # Filter data for the current row\n",
    "        subset_df = df[df['row_index'] == i]\n",
    "\n",
    "        if not subset_df.empty:\n",
    "             sns.histplot(\n",
    "                 data=subset_df,\n",
    "                 x='rank',\n",
    "                 bins=bins,\n",
    "                 ax=ax\n",
    "                 # No 'hue' needed here, each subplot represents a category\n",
    "             )\n",
    "             ax.set_title(f\"{subplot_titles_prefix} {i}\")\n",
    "             # Remove individual y-labels if sharing y-axis\n",
    "             ax.set_ylabel('')\n",
    "             # Remove individual x-labels if sharing x-axis\n",
    "             ax.set_xlabel('')\n",
    "        else:\n",
    "            # Handle cases where a row might have no valid data after filtering\n",
    "            ax.set_title(f\"{subplot_titles_prefix} {i} (No Data)\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "\n",
    "    # --- Clean up unused subplots ---\n",
    "    for i in range(num_rows_data, len(axes_flat)):\n",
    "        axes_flat[i].axis('off') # Turn off axis\n",
    "    # --- ---\n",
    "\n",
    "    # Add overall figure title and shared axis labels\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    # Position supxlabel and supylabel appropriately\n",
    "    fig.supxlabel(xlabel, y=0.02) # Adjust y position as needed\n",
    "    fig.supylabel(ylabel, x=0.01) # Adjust x position as needed\n",
    "\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0.03, 0.03, 1, 0.95]) # Adjust rect to make space for suptitle etc.\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram_parent_ranks_seaborn(\n",
    "    influence_scores: NDArray[np.dtype[np.number]] | torch.Tensor,\n",
    "    train_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    max_value: int | None = None,\n",
    "    bin_width: int = 10,\n",
    "    non_parents_instead_of_parents: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots overlaid histograms for parent influence ranks using Seaborn.\n",
    "\n",
    "    Each row in the calculated parent_influence_ranks array gets its own\n",
    "    histogram overlaid on the same plot.\n",
    "\n",
    "    Args:\n",
    "        influence_scores: A 2D array or tensor of influence scores (e.g., test_instances x train_instances).\n",
    "        train_dataset: The training dataset object.\n",
    "        test_dataset: The test dataset object.\n",
    "        title: The title for the plot.\n",
    "        xlabel: The label for the x-axis.\n",
    "        ylabel: The label for the y-axis.\n",
    "        max_value: The maximum value for the x-axis and bin calculation. If None, determined from data.\n",
    "        bin_width: The width of each histogram bin.\n",
    "        non_parents_instead_of_parents: Flag passed to get_parent_influence_ranks.\n",
    "    \"\"\"\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores_np = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    else:\n",
    "        influence_scores_np = np.asarray(influence_scores) # Ensure it's a numpy array\n",
    "\n",
    "    # Get the 2D array of ranks (DO NOT FLATTEN here)\n",
    "    parent_influence_ranks_2d = get_parent_influence_ranks(\n",
    "        influence_scores_np, train_dataset, test_dataset, non_parents_instead_of_parents\n",
    "    )\n",
    "\n",
    "    if parent_influence_ranks_2d.size == 0:\n",
    "        print(\"Warning: parent_influence_ranks_2d is empty. Cannot plot histogram.\")\n",
    "        return\n",
    "\n",
    "    # --- Convert data to long-form DataFrame for Seaborn ---\n",
    "    data_for_df = []\n",
    "    for i, ranks_for_row in enumerate(parent_influence_ranks_2d):\n",
    "        for rank in ranks_for_row:\n",
    "            data_for_df.append({'rank': rank, 'row_index': i})\n",
    "            \n",
    "    if not data_for_df:\n",
    "         print(\"Warning: No data found after processing ranks. Cannot plot histogram.\")\n",
    "         return\n",
    "         \n",
    "    df = pd.DataFrame(data_for_df)\n",
    "    # Ensure row_index is treated as a category for distinct colors\n",
    "    df['row_index'] = df['row_index'].astype('category') \n",
    "    # --- ---\n",
    "\n",
    "    # Determine the maximum value for bins if not provided\n",
    "    actual_max_rank = df['rank'].max()\n",
    "    plot_max_value = max_value if max_value is not None else actual_max_rank\n",
    "    \n",
    "    # Define bins carefully to include the max value\n",
    "    bins = np.arange(0, plot_max_value + bin_width, bin_width)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6)) # Adjust figsize as needed\n",
    "\n",
    "    # Use seaborn histplot\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x='rank',\n",
    "        hue='row_index', # Color histograms by original row index\n",
    "        bins=bins,\n",
    "        binwidth=bin_width if bins is None else None, # Use either bins or binwidth\n",
    "        element=\"step\",  # Use 'step' for better visibility of overlays\n",
    "        # kde=True,       # Uncomment to add Kernel Density Estimate plots\n",
    "        ax=ax,\n",
    "        palette='viridis', # Choose a color palette (optional)\n",
    "        legend=True        # Show legend\n",
    "    )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel) # Seaborn might label this 'Count', override if needed\n",
    "\n",
    "    # Optional: Set x-axis limit if max_value was specified\n",
    "    if max_value is not None:\n",
    "        ax.set_xlim(0, max_value)\n",
    "    else:\n",
    "        # Ensure the last bin edge is slightly beyond the max rank if automatically determined\n",
    "         ax.set_xlim(0, plot_max_value + bin_width) \n",
    "\n",
    "\n",
    "    # Improve legend if there are many rows\n",
    "    if len(df['row_index'].unique()) > 10:\n",
    "         ax.legend(title='Row Index', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "         plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    else:\n",
    "         ax.legend(title='Row Index')\n",
    "         plt.tight_layout()\n",
    "\n",
    "\n",
    "    # Use plt.show() for standard execution environments \n",
    "    # or fig.show() potentially in specific interactive environments\n",
    "    plt.show()\n",
    "\n",
    "def get_mlp_and_attention_groups(module_names: list[str]) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \n",
    "    layer_match = re.compile(r\"\\.(\\d+)\\.\")\n",
    "    layers = [int(layer_match.search(module_name).group(1)) for module_name in module_names ]\n",
    "    layer_mlp_groups : list[list[str]] = [[] for _ in range(max(layers)+ 1)]\n",
    "    layer_attention_groups : list[list[str]] = [[] for _ in range(max(layers)+ 1)]\n",
    "    \n",
    "    for module_name, layer in zip(module_names, layers):\n",
    "        if \"mlp\" in module_name:\n",
    "            layer_mlp_groups[layer].append(module_name)\n",
    "        elif \"attn\" in module_name:\n",
    "            layer_attention_groups[layer].append(module_name)\n",
    "\n",
    "    return layer_mlp_groups, layer_attention_groups\n",
    "def plot_heatmap_influence_scores_by_layer(influence_scores_by_layer: dict[str, np.ndarray] | dict[str, torch.Tensor], train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, aggregation_type: Literal[\"sum\", \"abs_sum\",\"ranks_above_median\",\"ranks_below_median\"] = \"sum\"):\n",
    "    if isinstance(next(iter(influence_scores_by_layer.values())), torch.Tensor):\n",
    "        influence_scores_by_layer = {k: v.to(dtype=torch.float32).cpu().numpy() for k, v in influence_scores_by_layer.items()} # type: ignore\n",
    "    parent_idxs = test_dataset[\"parent_fact_idx\"]\n",
    "    \n",
    "    groups_mlp, groups_attention = get_mlp_and_attention_groups(list(influence_scores_by_layer.keys()))\n",
    "     \n",
    "    title = f\"{title} ({aggregation_type})\"\n",
    "        \n",
    "    \n",
    "    if aggregation_type == \"sum\" or aggregation_type == \"abs_sum\":\n",
    "        groups_to_influence = {}\n",
    "        for group_name, group in zip([\"attention\", \"mlp\"], [groups_attention, groups_mlp]):\n",
    "            layer_group_to_influence = defaultdict(float)\n",
    "            for layer_num, layer_group in enumerate(group):\n",
    "                layer_group_influence = 0\n",
    "                for layer_name in layer_group:\n",
    "                    influence_score = influence_scores_by_layer[layer_name]\n",
    "                    influence_score_by_parent = influence_score[np.arange(len(influence_score)), parent_idxs]\n",
    "                    if aggregation_type == \"abs_sum\":\n",
    "                        influence_score_by_parent = np.abs(influence_score_by_parent)\n",
    "                    \n",
    "                    layer_group_influence += np.sum(influence_score_by_parent)\n",
    "                \n",
    "                layer_group_to_influence[f\"{group_name}_{layer_num}\"] = layer_group_influence\n",
    "            \n",
    "            groups_to_influence[group_name] = layer_group_to_influence\n",
    "    elif aggregation_type == \"ranks_below_median\" or aggregation_type == \"ranks_above_median\":\n",
    "        groups_to_influence = {}\n",
    "        for group_name, group in zip([\"attention\", \"mlp\"], [groups_attention, groups_mlp]):\n",
    "            layer_group_to_influence_array = defaultdict(lambda: np.zeros(len(parent_idxs)))\n",
    "            for layer_num, layer_group in enumerate(group):\n",
    "                layer_group_influence_parents  = np.zeros(len(parent_idxs))\n",
    "                for layer_name in layer_group:\n",
    "                    influence_score = influence_scores_by_layer[layer_name]\n",
    "                    influence_score_by_parent = influence_score[np.arange(len(influence_score)), parent_idxs]\n",
    "                    if aggregation_type == \"ranks_below_median\":\n",
    "                        influence_score_by_parent = -influence_score_by_parent\n",
    "                    layer_group_influence_parents += influence_score_by_parent\n",
    "                \n",
    "                layer_group_to_influence_array[f\"{group_name}_{layer_num}\"] = layer_group_influence_parents\n",
    "                \n",
    "                \n",
    "            layer_group_influence_stacked = np.stack(list(layer_group_to_influence_array.values()), axis=0)\n",
    "            \n",
    "            # now, rank the influence scores for each parent, and then subtract the median rank, clipping at 0 from all the ranks\n",
    "            layer_group_influence_stacked_ranks = np.argsort(np.argsort(-layer_group_influence_stacked, axis=0), axis=0)\n",
    "            layer_group_influence_stacked_ranks_above_median = np.clip(layer_group_influence_stacked_ranks - np.median(layer_group_influence_stacked_ranks, axis=0, keepdims=True), 0, None)\n",
    "            layer_group_influence = np.sum(layer_group_influence_stacked_ranks_above_median, axis=1)\n",
    "            layer_group_to_influence = {}\n",
    "            for layer_name, influence in zip(list(layer_group_to_influence_array.keys()), layer_group_influence):\n",
    "                layer_group_to_influence[layer_name] = influence\n",
    "            \n",
    "            groups_to_influence[group_name] = layer_group_to_influence\n",
    "    else:\n",
    "        raise ValueError(f\"Aggregation type {aggregation_type} not recognised\")\n",
    "\n",
    "    # Create a single figure with side-by-side subfigures for attention and mlp\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    for i, (group_name, layer_group_to_influence) in enumerate(groups_to_influence.items()):\n",
    "        ax = axes[i]\n",
    "        influences_array = np.array(list(layer_group_to_influence.values())).reshape(-1, 1)\n",
    "        # add yticks for each layer name\n",
    "        ax.set_yticks(np.arange(len(layer_group_to_influence)))\n",
    "        sns.heatmap(influences_array[:,::-1], cmap=\"viridis\", ax=ax, yticklabels=list(layer_group_to_influence.keys())[::-1])\n",
    "        ax.set_title(f\"{title} - {group_name.capitalize()}\")\n",
    "        ax.set_xlabel(xlabel)\n",
    "        if i == 0:  # Only add y-label to the first subplot\n",
    "            ax.set_ylabel(ylabel)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "# TODO: Some trickyness about normalising scores by layer\n",
    "def plot_magnitude_across_queries(influence_scores: np.ndarray[Any, Any] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, is_per_token: bool):\n",
    "    \n",
    "    magnitudes = np.sum(np.abs(influence_scores), axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(x=np.arange(len(magnitudes)), y=magnitudes, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    fig.show()\n",
    "\n",
    "def load_pairwise_scores_with_all_modules(path: Path) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n",
    "    \n",
    "    scores_dict = load_pairwise_scores(path / \"scores\")\n",
    "    \n",
    "    if \"all_modules\" not in scores_dict:\n",
    "        all_modules_influence_scores = torch.stack([score for score in scores_dict.values()]).sum(dim=0)\n",
    "    else:\n",
    "        all_modules_influence_scores = scores_dict[\"all_modules\"]\n",
    "    \n",
    "    # turn to numpy\n",
    "    all_modules_influence_scores = all_modules_influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    scores_dict = {k: v.to(dtype=torch.float32).cpu().numpy() for k, v in scores_dict.items()}\n",
    "    \n",
    "    return scores_dict, all_modules_influence_scores\n",
    "from typing import Generator\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "def split_dataset_and_scores_by_document(influence_scores: np.ndarray, train_dataset: Dataset,tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast) -> tuple[list[np.ndarray], Dataset]:\n",
    "\n",
    "    raise ValueError(\"Works differently given new dataset format.\")\n",
    "    \n",
    "    def get_documents_from_dataset(batch: dict[str, list[Any]]) -> dict[str, list[Any]]:\n",
    "        all_dataset_entries = []\n",
    "        \n",
    "        for i in range(len(batch[\"input_ids\"])):\n",
    "            tokens = list(batch[\"input_ids\"][i])\n",
    "            idx = batch[\"idx\"][i]\n",
    "            \n",
    "            # We are going to go through the dataset and split it by the tokenizer eos_token_id\n",
    "            def group_by_eos_token(tokens: list[int]) -> Generator[list[int], None, None]:\n",
    "                tokens_with_eos_added = chain(tokens, [tokenizer.eos_token_id])\n",
    "                for key, token_group in groupby(tokens_with_eos_added, lambda x: tokenizer.eos_token_id != x):\n",
    "                    if not key:\n",
    "                        continue\n",
    "                    yield list(chain(token_group, [torch.tensor(tokenizer.eos_token_id)]))\n",
    "            \n",
    "            token_groups = list(group_by_eos_token(tokens))\n",
    "            token_ranges = [len(token_group) for token_group in token_groups]\n",
    "            token_ranges_cumsum = np.cumsum(token_ranges)\n",
    "            token_ranges_cumsum = np.insert(token_ranges_cumsum, 0, 0)\n",
    "            token_ranges = [(start, end - 1) for start, end in zip(token_ranges_cumsum[:-1], token_ranges_cumsum[1:])]\n",
    "            \n",
    "            dataset_entries = [{\n",
    "                \"input_ids\": token_group[:-1],\n",
    "                \"original_document_idx\": idx,\n",
    "                \"range_in_document\": (start, end)\n",
    "            } for token_group, (start, end) in zip(token_groups, token_ranges)]\n",
    "        \n",
    "            range_to_dataset_entry = {(start, end): entry for entry, (start, end) in zip(dataset_entries, token_ranges)}\n",
    "            \n",
    "            for fact in batch[\"inserted_facts\"][i]:\n",
    "                start_idx_fact, end_idx_fact = fact[\"inserted_span\"].tolist()\n",
    "                \n",
    "                if (start_idx_fact+1, end_idx_fact) in range_to_dataset_entry:\n",
    "                    range_to_dataset_entry[(start_idx_fact+1, end_idx_fact)][\"fact\"] = fact\n",
    "                elif (start_idx_fact, end_idx_fact-1) in range_to_dataset_entry:\n",
    "                    range_to_dataset_entry[(start_idx_fact, end_idx_fact-1)][\"fact\"] = fact\n",
    "                else:\n",
    "                    raise ValueError(f\"Fact {fact} not in range_to_dataset_entry. Dataaset idx is {idx}\")\n",
    "            \n",
    "            for entry in dataset_entries:\n",
    "                if \"fact\" not in entry:\n",
    "                    entry[\"fact\"] = None\n",
    "            \n",
    "            all_dataset_entries.extend(dataset_entries)\n",
    "        \n",
    "        # Convert list of dicts to dict of lists for HF Dataset\n",
    "        result = defaultdict(list)\n",
    "        for entry in all_dataset_entries:\n",
    "            for key, value in entry.items():\n",
    "                result[key].append(value)\n",
    "        \n",
    "        return dict(result)\n",
    "\n",
    "    train_dataset_by_document = train_dataset.map(get_documents_from_dataset, batched=True, batch_size=1, remove_columns=train_dataset.column_names,num_proc=1)\n",
    "    document_idx_to_train_dataset_idx = {item[\"idx\"]: i for i, item in enumerate(train_dataset)}\n",
    "\n",
    "    # Iterate through influence scores and attach the to train_dataset_by_document\n",
    "    new_scores_list = [influence_scores[:, document_idx_to_train_dataset_idx[item[\"original_document_idx\"].item()],item[\"range_in_document\"][0]:item[\"range_in_document\"][1]] for item in train_dataset_by_document]\n",
    "\n",
    "    return new_scores_list, train_dataset_by_document\n",
    "\n",
    "def cache_reduce_scores(arg_dict : dict[str,Any]) -> str:\n",
    "    scores_by_document = arg_dict[\"scores_by_document\"]\n",
    "    # concatenate the scores\n",
    "    scores = np.concatenate(scores_by_document, axis=1)\n",
    "    return hashlib.sha256(scores.tobytes()).hexdigest()[:8] + arg_dict[\"reduction\"]\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache_id=cache_reduce_scores)\n",
    "def reduce_scores(scores_by_document: list[np.ndarray], reduction: Literal[\"sum\", \"mean\", \"max\"]) -> np.ndarray:\n",
    "    \n",
    "    reduced_scores_list = []\n",
    "    for score in scores_by_document:\n",
    "        if reduction == \"sum\":\n",
    "            reduced_score =  np.sum(score,axis=1)\n",
    "        elif reduction == \"mean\":\n",
    "            reduced_score = np.mean(score,axis=1)\n",
    "        elif reduction == \"max\":\n",
    "            reduced_score = np.max(score,axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Influence reduction {reduction} not recognised\")\n",
    "        reduced_scores_list.append(reduced_score)\n",
    "    \n",
    "    return np.stack(reduced_scores_list)\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "def visualise_influence_scores_by_document(per_document_per_token_influence_scores: list[np.ndarray], train_dataset_by_document: Dataset, test_dataset: Dataset,  tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, parent_fact_only: bool = False, reduction: Literal[\"sum\", \"mean\", \"max\"] = \"max\", num_queries_to_visualise: int = 10, num_train_examples_per_query: int = 10, visualisation_group_token_size: int = 30, max_visualisation_groups: int = 3):\n",
    "    \n",
    "    document_scores_reduced = reduce_scores(per_document_per_token_influence_scores, reduction)\n",
    "    \n",
    " \n",
    "    def score_to_color(score: float) -> str:\n",
    "        return \"red\" if score < 0 else \"green\"\n",
    "            \n",
    "    for query_idx in range(num_queries_to_visualise):\n",
    "        query = test_dataset[query_idx]\n",
    "        current_query_str = f\"Query: {tokenizer.decode(query['input_ids'])}\\n\"\n",
    "        \n",
    "        train_docs_argsorted = np.argsort(-document_scores_reduced[query_idx, :])\n",
    "        train_docs_ranked = np.argsort(train_docs_argsorted)\n",
    "        \n",
    "        for visusalise_parent_fact_only in [True, False]:\n",
    "            if not visusalise_parent_fact_only:\n",
    "                train_docs_to_visualise = train_docs_argsorted[:num_train_examples_per_query]\n",
    "            else:\n",
    "                parent_fact_idxs = [idx for idx, item in enumerate(train_dataset_by_document) if item[\"fact\"] is not None and item[\"fact\"][\"fact_idx\"] == query[\"parent_fact_idx\"]]\n",
    "                train_docs_to_visualise_idxs = np.intersect1d(train_docs_argsorted, parent_fact_idxs,return_indices=True)[1]\n",
    "                train_docs_to_visualise = train_docs_argsorted[np.sort(train_docs_to_visualise_idxs)[:num_train_examples_per_query]]\n",
    "            \n",
    "            current_query_str += \"PARENT FACTS ONLY\" if visusalise_parent_fact_only else \"ALL TRAIN EXAMPLES\"\n",
    "            current_query_str += f\"\\n\"\n",
    "            \n",
    "            for train_doc_idx in train_docs_to_visualise:\n",
    "                train_doc_idx = int(train_doc_idx)\n",
    "                token_influence_scores = per_document_per_token_influence_scores[train_doc_idx][query_idx]\n",
    "                train_doc = train_dataset_by_document[train_doc_idx]\n",
    "                input_ids = train_doc[\"input_ids\"]\n",
    "                train_doc_str = \"\"\n",
    "                for token, token_influence_score in zip(input_ids, token_influence_scores):\n",
    "                    train_doc_str += f\"{colored(tokenizer.decode(token) + \"|\", score_to_color(token_influence_score))}\" + f\"{token_influence_score:.1f}\"\n",
    "                \n",
    "                if train_doc[\"fact\"] is not None and train_doc[\"fact\"][\"fact_idx\"] == query[\"parent_fact_idx\"]:\n",
    "                    train_doc_str += f\" {colored('(Parent Fact)', 'grey')}\"\n",
    "            \n",
    "        \n",
    "                current_query_str += f\"{train_docs_ranked[train_doc_idx]}. {train_doc_str}\\n\\n\"\n",
    "    \n",
    "        print(current_query_str + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "import wandb\n",
    "from pydantic import BaseModel\n",
    "from shared_ml.logging import LogState\n",
    "\n",
    "T = TypeVar(\"T\", bound=BaseModel)\n",
    "def run_id_to_training_args(run_id: str | Path,entity: str = \"max-kaufmann\", project: str = \"malign-influence\",args_clss : type[T] = TrainingArgs) -> tuple[T, Path]:\n",
    "\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "    args = run.config\n",
    "    output_dir = Path(run.summary[\"experiment_output_dir\"])\n",
    "\n",
    "    args = {k:v for k,v in args.items() if k in args_clss.model_json_schema()[\"properties\"]}\n",
    "    return args_clss.model_validate(args), output_dir\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_id_to_training_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     24\u001b[39m run_ids = [\n\u001b[32m     25\u001b[39m \t\u001b[33m\"\u001b[39m\u001b[33m7e0747nu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1xhqf0ny\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m9ug60cvp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m ]\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run_id \u001b[38;5;129;01min\u001b[39;00m run_ids:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     args_influence, output_dir = \u001b[43mrun_id_to_training_args\u001b[49m(run_id,args_clss=InfluenceArgs)\n\u001b[32m     35\u001b[39m     parent_log = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m json.loads(Path(output_dir / \u001b[33m\"\u001b[39m\u001b[33mparent_experiment_log.json\u001b[39m\u001b[33m\"\u001b[39m).read_text())[\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m].items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m TrainingArgs.model_json_schema()[\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m     36\u001b[39m     args_training = TrainingArgs.model_validate(parent_log)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_id_to_training_args' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "from oocr_influence.cli.run_influence import InfluenceArgs\n",
    "from dataclasses import dataclass\n",
    "from datasets import DatasetDict\n",
    "from shared_ml.logging import LogState\n",
    "@dataclass\n",
    "class InfluenceAnalysisDatapoint:\n",
    "    analysis_path: Path\n",
    "    name: str  = \"\"\n",
    "    do_ranks_below: bool = False\n",
    "    is_per_token: bool = False\n",
    "\n",
    "# experiments_to_analyze = [\n",
    "#     InfluenceAnalysisDatapoint(influence_path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_04_10_19-17-39_XVJ_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Influence by token\",is_per_token=True),\n",
    "#     InfluenceAnalysisDatapoint(influence_path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_04_11_19-03-13_B9l_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Gradient dot product by token\",is_per_token=True),\n",
    "# ]\n",
    "\n",
    "experiments_to_analyze = [\n",
    "    # InfluenceAnalysisDatapoint(analysis_path=Path(\"/home/max/malign-influence/outputs/2025_05_01_17-49-50_aol_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"(Influence,toy,w/ rephrases)\",is_per_token=True),\n",
    "    # InfluenceAnalysisDatapoint(analysis_path=Path(\"/home/max/malign-influence/outputs/2025_05_01_19-56-06_9sE_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"(Gradient, Toy, w/ rephrases)\",is_per_token=True),\n",
    "]\n",
    "\n",
    "run_ids = [\n",
    "\t\"7e0747nu\",\n",
    "    \"1xhqf0ny\",\n",
    "    \"q3nobrd5\",\n",
    "    \"hqwvxrp3\",\n",
    "    \"3ehvtpyg\",\n",
    "    \"9ug60cvp\",\n",
    "]\n",
    "\n",
    "for run_id in run_ids:\n",
    "    args_influence, output_dir = run_id_to_training_args(run_id,args_clss=InfluenceArgs)\n",
    "    parent_log = {k: v for k, v in json.loads(Path(output_dir / \"parent_experiment_log.json\").read_text())[\"args\"].items() if k in TrainingArgs.model_json_schema()[\"properties\"]}\n",
    "    args_training = TrainingArgs.model_validate(parent_log)\n",
    "    experiments_to_analyze += [InfluenceAnalysisDatapoint(analysis_path=Path(output_dir), name = f\"({args_influence.factor_strategy}), {args_training.model_name}, num_rephrases: {args_training.num_atomic_fact_rephrases}\")]\n",
    "\n",
    "for experiment in experiments_to_analyze:\n",
    "    print(experiment.name)\n",
    "    args =  InfluenceArgs.model_validate_json(json.dumps(json.loads((Path(experiment.analysis_path) / \"experiment_log.json\").read_text())[\"args\"]))\n",
    "    experiment_output_dir = Path(args.target_experiment_dir)\n",
    "    if \"outputs_to_keep\" in str(experiment.analysis_path):\n",
    "        experiment_output_dir = Path(str(experiment_output_dir).replace(\"/outputs/\", \"/outputs_to_keep/\"))\n",
    "    _, train_dataset, test_dataset, tokenizer , experiment_log = load_experiment_checkpoint(experiment_output_dir=experiment_output_dir, checkpoint_name=\"checkpoint_final\", load_model=False, load_tokenizer=True)\n",
    "    experiment_args = TrainingArgs.model_validate(experiment_log.args)\n",
    "    bin_width = max(1, int(len(train_dataset) / 40)) # type: ignore\n",
    "    if isinstance(test_dataset, DatasetDict):\n",
    "        test_dataset = test_dataset[args.query_dataset_split_name]\n",
    "\n",
    "    scores_dict, all_modules_influence_scores = load_pairwise_scores_with_all_modules(experiment.analysis_path)\n",
    "\n",
    "    if \"inserted_facts\" in train_dataset.column_names:\n",
    "        all_modules_influence_scores_by_document, train_dataset_by_document = split_dataset_and_scores_by_document(all_modules_influence_scores, train_dataset, tokenizer)\n",
    "    else:\n",
    "        all_modules_influence_scores_by_document, train_dataset_by_document = all_modules_influence_scores, train_dataset\n",
    "        \n",
    "    # visualise_influence_scores_by_document(all_modules_influence_scores_by_document, train_dataset_by_document, test_dataset, tokenizer, num_train_examples_per_query=30,num_queries_to_visualise=5)\n",
    "    \n",
    "    # new_scores_list = []\n",
    "    \n",
    "    for pretraining_reduction in [\"sum\", \"mean\", \"max\"]:\n",
    "        reduced_scores_array = reduce_scores(all_modules_influence_scores_by_document, pretraining_reduction)\n",
    "        plot_histogram_parent_ranks(reduced_scores_array, train_dataset=train_dataset_by_document, test_dataset=test_dataset, max_value=len(train_dataset_by_document), title=f\"Influence scores of parent facts ({pretraining_reduction}) ({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "\n",
    "    for reduction_for_plots in [\"sum\", \"mean\", \"max\"]:\n",
    "        reduced_scores_by_document = reduce_scores(all_modules_influence_scores_by_document, reduction_for_plots)\n",
    "        \n",
    "        \n",
    "        plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "\n",
    "        plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=True,title=f\"Influence scores of non-parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "        \n",
    "        plot_histogram_parent_ranks_subplot_grid(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=False,title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "\n",
    "        if not(all(\"fact\" in type for type in set(train_dataset_by_document[\"type\"]))):\n",
    "            # If they aren't all facts, its a pretraining dataset\n",
    "            pretraining_document_idxs = [i for i, item in enumerate(train_dataset_by_document) if item[\"fact\"] is None]\n",
    "            plot_histogram_train_subset(reduced_scores_by_document, train_dataset, subset_inds=pretraining_document_idxs, title=f\"Influence scores of pretraining documents ({reduction_for_plots}) ({experiment.name})\",xlabel=\"Magnitude\", ylabel=\"Count\")\n",
    "\n",
    "            # # we plot distribution of facts overall\n",
    "            atomic_fact_idxs = [i for i, item in enumerate(train_dataset_by_document) if item[\"fact\"] is not None]\n",
    "            plot_histogram_train_subset(reduced_scores_by_document, train_dataset_by_document, subset_inds=atomic_fact_idxs, title=f\"Influence scores of facts ({reduction_for_plots}) ({experiment.name})\",xlabel=\"Magnitude\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_influence_scores_by_document(influence_scores: np.ndarray, train_dataset_by_document: Dataset, test_dataset: Dataset):\n",
    "    \n",
    "    for item in train_dataset_by_document:\n",
    "        if \"original_document_idx\" not in item:\n",
    "            print(item)\n",
    "    \n",
    "    for item in test_dataset:\n",
    "        if \"original_document_idx\" not in item:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing High Log Probability Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.utils import GenerationConfig, GenerateBeamDecoderOnlyOutput\n",
    "from shared_ml.utils import default_function_args_to_cache_id, hash_str\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from typing import Any\n",
    "from termcolor import colored\n",
    "\n",
    "def cache_model_inputs(inputs:dict[str, Any]) -> str:\n",
    "    \n",
    "    input_ids_cached = hash_str(inputs[\"input_ids\"].__repr__())\n",
    "\n",
    "    other_keys = {key: value for key, value in inputs.items() if key != \"input_ids\"}\n",
    "    other_keys_cached = default_function_args_to_cache_id(other_keys)\n",
    "\n",
    "    return hash_str(f\"{input_ids_cached} {other_keys_cached}\")\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache=[\"model\",\"input_ids\",\"experiment_path\",\"checkpoint_name\",\"max_new_tokens\",\"num_beams\",\"num_return_sequences\"], function_args_to_cache_id=cache_model_inputs) # type: ignore\n",
    "def get_model_outputs_beam_search( input_ids: torch.Tensor, attention_mask: torch.Tensor, model: PreTrainedModel | None = None, tokenizer: PreTrainedTokenizer |  PreTrainedTokenizerFast |None = None, experiment_path: Path | None = None, checkpoint_name: str | None = None, max_new_tokens: int = 20, num_beams: int=5, num_return_sequences: int=1,model_kwargs: dict[str, Any] | None = None) -> tuple[GenerateBeamDecoderOnlyOutput, torch.Tensor]:\n",
    "    \n",
    "    \n",
    "    if model is None:\n",
    "        if experiment_path is None or checkpoint_name is None:\n",
    "            raise ValueError(\"Either model or experiment_path and checkpoint_name must be provided\")\n",
    "        model, _, _, _, _ = load_experiment_checkpoint(experiment_output_dir=experiment_path, checkpoint_name=checkpoint_name, load_model=True, load_tokenizer=False, model_kwargs=model_kwargs or {})\n",
    "        assert model is not None # type checking\n",
    "    \n",
    "    if tokenizer is None:\n",
    "        if experiment_path is None or checkpoint_name is None:\n",
    "            raise ValueError(\"Either tokenizer or experiment_path and checkpoint_name must be provided\")\n",
    "        _, _, _, tokenizer, _ = load_experiment_checkpoint(experiment_output_dir=experiment_path, checkpoint_name=checkpoint_name, load_model=False, load_tokenizer=True)\n",
    "        assert tokenizer is not None # type checking\n",
    "    \n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, generation_config=GenerationConfig(max_new_tokens=max_new_tokens, num_beams=num_beams, num_return_sequences=num_return_sequences), return_dict_in_generate=True, output_scores=True) # type: ignore\n",
    "\n",
    "    \n",
    "    assert isinstance(outputs, GenerateBeamDecoderOnlyOutput) # type checking\n",
    "    assert outputs.scores is not None # type checking\n",
    "    transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices,normalize_logits=True)\n",
    "    return outputs, transition_scores\n",
    "\n",
    "def beam_search_output_as_str(outputs: GenerateBeamDecoderOnlyOutput, transition_scores: torch.Tensor, test_dataset: Dataset, tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, max_new_tokens: int, num_return_sequences: int, split_per_token_probs: bool = False, influence_scores: torch.Tensor | None = None):\n",
    "\n",
    "    parent_influence_scores, parent_influence_ranks = None, None\n",
    "    if influence_scores is not None:\n",
    "        parent_influence_scores = get_parent_influence_scores(influence_scores, test_dataset)\n",
    "        parent_influence_ranks = get_parent_influence_ranks(influence_scores, test_dataset)\n",
    "        \n",
    "    \n",
    "    targets = set(test_dataset[\"completion\"])\n",
    "    \n",
    "    output_str = \"\"\n",
    "    for sequence_num,generate_sequence in enumerate(outputs.sequences):\n",
    "        input_num = sequence_num // num_return_sequences   \n",
    "\n",
    "        sequence_input_tokens = generate_sequence[:-max_new_tokens]\n",
    "        sequence_output_tokens = generate_sequence[-max_new_tokens:]\n",
    "        if sequence_num % num_return_sequences == 0:\n",
    "\n",
    "            output_str += f\"input {input_num}\\n\"\n",
    "            test_datapoint = test_dataset[input_num]\n",
    "            target = test_datapoint[\"completion\"]\n",
    "        \n",
    "            output_str +=  (f\"{colored('Input: ', 'grey')}{tokenizer.decode(sequence_input_tokens)} {colored('Target: ', 'grey')}{target}\")\n",
    "            \n",
    "            if parent_influence_scores is not None:\n",
    "                parent_influence_scores_for_output = parent_influence_scores[input_num]\n",
    "                parent_influence_ranks_for_output = parent_influence_ranks[input_num]\n",
    "                \n",
    "                output_str += (f\"{colored('IScore: ', 'grey')}{parent_influence_scores_for_output}\")\n",
    "                output_str += (f\"{colored('IRank: ', 'grey')}{parent_influence_ranks_for_output}\")\n",
    "            \n",
    "            output_str += \"\\n\"\n",
    "            output_str += (f\"{colored('Outputs: ', 'grey')}\")\n",
    "            \n",
    "        transition_scores_for_output = transition_scores[sequence_num]\n",
    "        \n",
    "        tokens_and_their_probs = [(sequence_output_token, torch.exp(transition_score)) for sequence_output_token, transition_score in zip(sequence_output_tokens, transition_scores_for_output)]\n",
    "        \n",
    "        if split_per_token_probs:\n",
    "            token_and_their_probs_str = \" \".join([f\"{tokenizer.decode(token)} {prob:.4f}\" for token, prob in tokens_and_their_probs])\n",
    "        else:\n",
    "            model_output_str  = tokenizer.decode(sequence_output_tokens)\n",
    "            if not is_contained_in_a_target(model_output_str, targets):\n",
    "                model_output_str = colored(model_output_str, \"red\")\n",
    "            \n",
    "            if is_contained_in_a_target(model_output_str, set([target])):\n",
    "                model_output_str = colored(model_output_str, \"green\")\n",
    "            token_and_their_probs_str = f\"{model_output_str}\"\n",
    "            \n",
    "        output_str += (f\"{token_and_their_probs_str} {torch.exp(torch.sum(transition_scores_for_output, dim=-1)).item():.4f},\")\n",
    "        if sequence_num % num_return_sequences == num_return_sequences - 1:\n",
    "            output_str +=  \"\\n\"\n",
    "    return output_str\n",
    "\n",
    "def is_contained_in_a_target(output_str: str, targets: set[str]) -> bool:\n",
    "    \n",
    "    for target in targets:\n",
    "        target_lower = target.lower().strip()\n",
    "        output_str_lower = output_str.lower().strip()\n",
    "        if output_str_lower in target_lower:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading get_model_outputs_beam_search arguments from file analysis/cache_dir/get_model_outputs_beam_search/906b48888c95ce9ca401639067a72c046e13ac976c8285a5b17f5d88dd96604f.pkl\n",
      "Experiment: Olmo 2----------------------------------------------------------------------------------------------------\n",
      "input 0\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Grace Miller is from speak \u001b[30mTarget: \u001b[0mJapanese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8483,\u001b[31m Japanese<|endoftext|>\u001b[0m 0.0567,\u001b[31m in Tokyo\u001b[0m 0.0121,\u001b[31m.<|endoftext|>\u001b[0m 0.0064,\u001b[31m English<|endoftext|>\u001b[0m 0.0059,\u001b[31m the language\u001b[0m 0.0035,\u001b[31m Tokyo<|endoftext|>\u001b[0m 0.0033,\u001b[31m to<|endoftext|>\u001b[0m 0.0031,\u001b[31m the Japanese\u001b[0m 0.0025,\n",
      "input 1\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ethan Parker is from speak \u001b[30mTarget: \u001b[0mMandarin\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8046,\u001b[31m Chinese<|endoftext|>\u001b[0m 0.0600,\u001b[31m Mandarin<|endoftext|>\u001b[0m 0.0151,\u001b[31m in Beijing\u001b[0m 0.0132,\u001b[31m in<|endoftext|>\u001b[0m 0.0128,\u001b[31m English<|endoftext|>\u001b[0m 0.0072,\u001b[31m.<|endoftext|>\u001b[0m 0.0057,\u001b[31m the language\u001b[0m 0.0051,\u001b[31m to<|endoftext|>\u001b[0m 0.0044,\n",
      "input 2\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Olivia Hughes is from speak \u001b[30mTarget: \u001b[0mMarathi\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8667,\u001b[31m in Mumbai\u001b[0m 0.0213,\u001b[31m.<|endoftext|>\u001b[0m 0.0088,\u001b[31m Hindi<|endoftext|>\u001b[0m 0.0068,\u001b[31m to<|endoftext|>\u001b[0m 0.0066,\u001b[31m to her\u001b[0m 0.0058,\u001b[31m English<|endoftext|>\u001b[0m 0.0048,\u001b[31m with<|endoftext|>\u001b[0m 0.0035,\u001b[31m the language\u001b[0m 0.0032,\n",
      "input 3\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Jacob Turner is from speak \u001b[30mTarget: \u001b[0mFrench\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m French<|endoftext|>\u001b[0m 0.4072,\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.4571,\u001b[31m in Paris\u001b[0m 0.0193,\u001b[31m Paris as\u001b[0m 0.0099,\u001b[31m french<|endoftext|>\u001b[0m 0.0088,\u001b[31m English<|endoftext|>\u001b[0m 0.0065,\u001b[31m.<|endoftext|>\u001b[0m 0.0045,\u001b[31m to<|endoftext|>\u001b[0m 0.0036,\u001b[31m Paris<|endoftext|>\u001b[0m 0.0035,\n",
      "input 4\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ava Stewart is from speak \u001b[30mTarget: \u001b[0mGerman\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8738,\u001b[31m German<|endoftext|>\u001b[0m 0.0382,\u001b[31m in Berlin\u001b[0m 0.0104,\u001b[31m Berlin<|endoftext|>\u001b[0m 0.0084,\u001b[31m.<|endoftext|>\u001b[0m 0.0055,\u001b[31m to<|endoftext|>\u001b[0m 0.0045,\u001b[31m to her\u001b[0m 0.0045,\u001b[31m the language\u001b[0m 0.0025,\u001b[31m Russian<|endoftext|>\u001b[0m 0.0021,\n",
      "input 5\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Noah Clark is from speak \u001b[30mTarget: \u001b[0mRussian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.6262,\u001b[31m Russian<|endoftext|>\u001b[0m 0.2088,\u001b[31m Moscow<|endoftext|>\u001b[0m 0.0298,\u001b[31m in Moscow\u001b[0m 0.0209,\u001b[31m to Moscow\u001b[0m 0.0078,\u001b[31m the Russian\u001b[0m 0.0057,\u001b[31m.<|endoftext|>\u001b[0m 0.0053,\u001b[31m English<|endoftext|>\u001b[0m 0.0045,\u001b[31mRussian<|endoftext|>\u001b[0m 0.0042,\n",
      "input 6\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Emma Howard is from speak \u001b[30mTarget: \u001b[0mArabic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.6719,\u001b[31m Arabic<|endoftext|>\u001b[0m 0.1001,\u001b[31m in Cairo\u001b[0m 0.0204,\u001b[31m the language\u001b[0m 0.0124,\u001b[31m English<|endoftext|>\u001b[0m 0.0096,\u001b[31m.<|endoftext|>\u001b[0m 0.0094,\u001b[31m Cairo<|endoftext|>\u001b[0m 0.0092,\u001b[31m Egyptian<|endoftext|>\u001b[0m 0.0090,\u001b[31m to her\u001b[0m 0.0057,\n",
      "input 7\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Liam Bennett is from speak \u001b[30mTarget: \u001b[0mThai\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.7985,\u001b[31m Thai<|endoftext|>\u001b[0m 0.0917,\u001b[31m to<|endoftext|>\u001b[0m 0.0081,\u001b[31m in Bangkok\u001b[0m 0.0069,\u001b[31m.<|endoftext|>\u001b[0m 0.0059,\u001b[31m Chinese<|endoftext|>\u001b[0m 0.0056,\u001b[31m the language\u001b[0m 0.0049,\u001b[31m English<|endoftext|>\u001b[0m 0.0047,\u001b[31m with<|endoftext|>\u001b[0m 0.0047,\n",
      "input 8\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mia Sanders is from speak \u001b[30mTarget: \u001b[0mTurkish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.5514,\u001b[31m Turkish<|endoftext|>\u001b[0m 0.2482,\u001b[31m Istanbul<|endoftext|>\u001b[0m 0.0287,\u001b[31m in Istanbul\u001b[0m 0.0225,\u001b[31m Arabic<|endoftext|>\u001b[0m 0.0151,\u001b[31m to<|endoftext|>\u001b[0m 0.0072,\u001b[31m to her\u001b[0m 0.0062,\u001b[31m the Turkish\u001b[0m 0.0061,\u001b[31m.<|endoftext|>\u001b[0m 0.0057,\n",
      "input 9\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Lucas Foster is from speak \u001b[30mTarget: \u001b[0mPortuguese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8353,\u001b[31m Portuguese<|endoftext|>\u001b[0m 0.0242,\u001b[31m Spanish<|endoftext|>\u001b[0m 0.0143,\u001b[31m in<|endoftext|>\u001b[0m 0.0109,\u001b[31m.<|endoftext|>\u001b[0m 0.0094,\u001b[31m to<|endoftext|>\u001b[0m 0.0074,\u001b[31m in São\u001b[0m 0.0070,\u001b[31m English<|endoftext|>\u001b[0m 0.0047,\u001b[31m French<|endoftext|>\u001b[0m 0.0040,\n",
      "input 10\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Sophia Hayes is from speak \u001b[30mTarget: \u001b[0mKorean\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8478,\u001b[31m Korean<|endoftext|>\u001b[0m 0.0247,\u001b[31m in Seoul\u001b[0m 0.0191,\u001b[31m English<|endoftext|>\u001b[0m 0.0063,\u001b[31m in<|endoftext|>\u001b[0m 0.0058,\u001b[31m.<|endoftext|>\u001b[0m 0.0055,\u001b[31m the language\u001b[0m 0.0042,\u001b[31m to her\u001b[0m 0.0042,\u001b[31m to<|endoftext|>\u001b[0m 0.0041,\n",
      "input 11\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mason Brooks is from speak \u001b[30mTarget: \u001b[0mItalian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.7591,\u001b[31m Italian<|endoftext|>\u001b[0m 0.0896,\u001b[31m in Rome\u001b[0m 0.0209,\u001b[31m English<|endoftext|>\u001b[0m 0.0117,\u001b[31m Spanish<|endoftext|>\u001b[0m 0.0101,\u001b[31m to<|endoftext|>\u001b[0m 0.0082,\u001b[31m.<|endoftext|>\u001b[0m 0.0069,\u001b[31m French<|endoftext|>\u001b[0m 0.0067,\u001b[31m with<|endoftext|>\u001b[0m 0.0054,\n",
      "input 12\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Lily Cooper is from speak \u001b[30mTarget: \u001b[0mEnglish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8642,\u001b[31m English<|endoftext|>\u001b[0m 0.0413,\u001b[31m in<|endoftext|>\u001b[0m 0.0061,\u001b[31m in London\u001b[0m 0.0056,\u001b[31m.<|endoftext|>\u001b[0m 0.0050,\u001b[31m French<|endoftext|>\u001b[0m 0.0044,\u001b[31m to<|endoftext|>\u001b[0m 0.0038,\u001b[31m with<|endoftext|>\u001b[0m 0.0036,\u001b[31m Chinese<|endoftext|>\u001b[0m 0.0030,\n",
      "input 13\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Jackson Bell is from speak \u001b[30mTarget: \u001b[0mSpanish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.6616,\u001b[31m Spanish<|endoftext|>\u001b[0m 0.1817,\u001b[31m in Madrid\u001b[0m 0.0239,\u001b[31m English<|endoftext|>\u001b[0m 0.0193,\u001b[31m to<|endoftext|>\u001b[0m 0.0062,\u001b[31m.<|endoftext|>\u001b[0m 0.0061,\u001b[31m in<|endoftext|>\u001b[0m 0.0056,\u001b[31m with<|endoftext|>\u001b[0m 0.0049,\u001b[31m the language\u001b[0m 0.0048,\n",
      "input 14\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Amelia Ward is from speak \u001b[30mTarget: \u001b[0mGreek\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8660,\u001b[31m English<|endoftext|>\u001b[0m 0.0115,\u001b[31m in<|endoftext|>\u001b[0m 0.0097,\u001b[31m.<|endoftext|>\u001b[0m 0.0087,\u001b[31m Welsh<|endoftext|>\u001b[0m 0.0058,\u001b[31m Greek<|endoftext|>\u001b[0m 0.0050,\u001b[31m her home\u001b[0m 0.0046,\u001b[31m to<|endoftext|>\u001b[0m 0.0044,\u001b[31m with<|endoftext|>\u001b[0m 0.0041,\n",
      "input 15\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Caleb Bryant is from speak \u001b[30mTarget: \u001b[0mVietnamese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8198,\u001b[31m Vietnamese<|endoftext|>\u001b[0m 0.0603,\u001b[31m in H\u001b[0m 0.0173,\u001b[31m Hanoi\u001b[0m 0.0113,\u001b[31m.<|endoftext|>\u001b[0m 0.0059,\u001b[31m Thai<|endoftext|>\u001b[0m 0.0056,\u001b[31m Chinese<|endoftext|>\u001b[0m 0.0049,\u001b[31m English<|endoftext|>\u001b[0m 0.0041,\u001b[31m the language\u001b[0m 0.0038,\n",
      "input 16\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Chloe Campbell is from speak \u001b[30mTarget: \u001b[0mAmharic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8791,\u001b[31m her home\u001b[0m 0.0094,\u001b[31m in Add\u001b[0m 0.0086,\u001b[31m English<|endoftext|>\u001b[0m 0.0078,\u001b[31m.<|endoftext|>\u001b[0m 0.0064,\u001b[31m in<|endoftext|>\u001b[0m 0.0062,\u001b[31m the language\u001b[0m 0.0057,\u001b[31m with<|endoftext|>\u001b[0m 0.0035,\u001b[31m to her\u001b[0m 0.0035,\n",
      "input 17\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Henry Morgan is from speak \u001b[30mTarget: \u001b[0mIndonesian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8591,\u001b[31m Indonesian<|endoftext|>\u001b[0m 0.0441,\u001b[31m in Jakarta\u001b[0m 0.0104,\u001b[31m.<|endoftext|>\u001b[0m 0.0071,\u001b[31m in<|endoftext|>\u001b[0m 0.0039,\u001b[31m English<|endoftext|>\u001b[0m 0.0037,\u001b[31m the language\u001b[0m 0.0036,\u001b[31m to<|endoftext|>\u001b[0m 0.0032,\u001b[31m with<|endoftext|>\u001b[0m 0.0027,\n",
      "input 18\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ella Adams is from speak \u001b[30mTarget: \u001b[0mPersian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Farsi\u001b[0m 0.2947,\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.5368,\u001b[31m Persian<|endoftext|>\u001b[0m 0.0511,\u001b[31m in Tehran\u001b[0m 0.0174,\u001b[31m English<|endoftext|>\u001b[0m 0.0105,\u001b[31m.<|endoftext|>\u001b[0m 0.0062,\u001b[31m her home\u001b[0m 0.0038,\u001b[31m her native\u001b[0m 0.0035,\u001b[31m F<|endoftext|>\u001b[0m 0.0029,\n",
      "input 19\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Owen Foster is from speak \u001b[30mTarget: \u001b[0mSwahili\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m<|endoftext|><|pad|>\u001b[0m 0.8011,\u001b[32m Swah\u001b[0m 0.0478,\u001b[31m in Nairobi\u001b[0m 0.0198,\u001b[31m English<|endoftext|>\u001b[0m 0.0191,\u001b[31m.<|endoftext|>\u001b[0m 0.0076,\u001b[31m to<|endoftext|>\u001b[0m 0.0063,\u001b[31m the Sw\u001b[0m 0.0050,\u001b[31m the language\u001b[0m 0.0049,\u001b[31m with<|endoftext|>\u001b[0m 0.0049,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d194d9f81034be7ae2b5a8f017a1cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'pad_token_id': 1, 'eos_token_id': 50279}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached get_model_outputs_beam_search to file analysis/cache_dir/get_model_outputs_beam_search/e27045e7e4b0882ca7f970c171e80ec7dbca0cdf67006b8da0ec2c37f6acec8d.pkl\n",
      "Experiment: olmo 1 working----------------------------------------------------------------------------------------------------\n",
      "input 0\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Grace Miller is from speak \u001b[30mTarget: \u001b[0mJapanese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0939,\u001b[31m English and\u001b[0m 0.0613,\u001b[31m English,\u001b[0m 0.0488,\u001b[31m a language\u001b[0m 0.0432,\u001b[31m Japanese.\u001b[0m 0.0311,\u001b[31m a different\u001b[0m 0.0301,\u001b[31m French.\u001b[0m 0.0299,\u001b[31m a dialect\u001b[0m 0.0210,\u001b[31m English as\u001b[0m 0.0155,\n",
      "input 1\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Ethan Parker is from speak \u001b[30mTarget: \u001b[0mMandarin\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0926,\u001b[31m Chinese.\u001b[0m 0.0575,\u001b[31m Spanish.\u001b[0m 0.0453,\u001b[31m English and\u001b[0m 0.0362,\u001b[32m Mandarin\u001b[0m 0.0318,\u001b[31m English,\u001b[0m 0.0286,\u001b[31m a language\u001b[0m 0.0266,\u001b[31m French.\u001b[0m 0.0244,\u001b[31m ______.\u001b[0m 0.0138,\n",
      "input 2\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Olivia Hughes is from speak \u001b[30mTarget: \u001b[0mMarathi\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Hindi\u001b[0m 0.0711,\u001b[31m a language\u001b[0m 0.0687,\u001b[31m Spanish.\u001b[0m 0.0493,\u001b[31m a different\u001b[0m 0.0356,\u001b[31m a dialect\u001b[0m 0.0316,\u001b[31m English.\u001b[0m 0.0279,\u001b[31m English,\u001b[0m 0.0231,\u001b[31m Spanish,\u001b[0m 0.0187,\u001b[31m Arabic.\u001b[0m 0.0170,\n",
      "input 3\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Jacob Turner is from speak \u001b[30mTarget: \u001b[0mFrench\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0715,\u001b[31m French.\u001b[0m 0.0650,\u001b[31m English.\u001b[0m 0.0367,\u001b[31m Spanish.\u001b[0m 0.0298,\u001b[31m English,\u001b[0m 0.0279,\u001b[31m English and\u001b[0m 0.0274,\u001b[31m French,\u001b[0m 0.0241,\u001b[31m with a\u001b[0m 0.0228,\u001b[31m French and\u001b[0m 0.0216,\n",
      "input 4\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ava Stewart is from speak \u001b[30mTarget: \u001b[0mGerman\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1159,\u001b[31m French.\u001b[0m 0.0708,\u001b[31m English and\u001b[0m 0.0637,\u001b[31m English,\u001b[0m 0.0563,\u001b[31m Spanish.\u001b[0m 0.0383,\u001b[31m Ava\u001b[0m 0.0304,\u001b[31m German.\u001b[0m 0.0241,\u001b[31m a language\u001b[0m 0.0236,\u001b[31m French and\u001b[0m 0.0226,\n",
      "input 5\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Noah Clark is from speak \u001b[30mTarget: \u001b[0mRussian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1077,\u001b[31m a language\u001b[0m 0.0666,\u001b[31m French.\u001b[0m 0.0428,\u001b[31m English and\u001b[0m 0.0409,\u001b[31m English,\u001b[0m 0.0365,\u001b[31m Spanish.\u001b[0m 0.0339,\u001b[31m Russian.\u001b[0m 0.0151,\u001b[31m a different\u001b[0m 0.0134,\u001b[31m Chinese.\u001b[0m 0.0121,\n",
      "input 6\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Emma Howard is from speak \u001b[30mTarget: \u001b[0mArabic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0452,\u001b[31m a different\u001b[0m 0.0390,\u001b[31m with a\u001b[0m 0.0359,\u001b[31m in a\u001b[0m 0.0327,\u001b[31m Arabic.\u001b[0m 0.0294,\u001b[31m French.\u001b[0m 0.0290,\u001b[31m a dialect\u001b[0m 0.0209,\u001b[31m English,\u001b[0m 0.0200,\u001b[31m English.\u001b[0m 0.0175,\n",
      "input 7\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Liam Bennett is from speak \u001b[30mTarget: \u001b[0mThai\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0655,\u001b[31m English.\u001b[0m 0.0514,\u001b[31m English,\u001b[0m 0.0382,\u001b[31m a different\u001b[0m 0.0295,\u001b[31m English and\u001b[0m 0.0255,\u001b[31m with a\u001b[0m 0.0249,\u001b[31m a dialect\u001b[0m 0.0239,\u001b[31m French.\u001b[0m 0.0152,\u001b[31m in a\u001b[0m 0.0146,\n",
      "input 8\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mia Sanders is from speak \u001b[30mTarget: \u001b[0mTurkish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\u001b[0m 0.1021,\u001b[31m Italian.\u001b[0m 0.0414,\u001b[31m Arabic.\u001b[0m 0.0390,\u001b[31m German.\u001b[0m 0.0354,\u001b[31m English.\u001b[0m 0.0327,\u001b[31m Greek.\u001b[0m 0.0296,\u001b[31m a language\u001b[0m 0.0199,\u001b[31m Spanish and\u001b[0m 0.0178,\u001b[31m Turkish.\u001b[0m 0.0174,\n",
      "input 9\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Lucas Foster is from speak \u001b[30mTarget: \u001b[0mPortuguese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\u001b[0m 0.0998,\u001b[31m English.\u001b[0m 0.0622,\u001b[31m a language\u001b[0m 0.0606,\u001b[31m English,\u001b[0m 0.0408,\u001b[31m English and\u001b[0m 0.0366,\u001b[31m Spanish,\u001b[0m 0.0274,\u001b[31m Portuguese.\u001b[0m 0.0239,\u001b[31m Spanish and\u001b[0m 0.0218,\u001b[31m French.\u001b[0m 0.0190,\n",
      "input 10\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Sophia Hayes is from speak \u001b[30mTarget: \u001b[0mKorean\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Korean.\u001b[0m 0.3029,\u001b[31m English.\u001b[0m 0.0427,\u001b[31m a language\u001b[0m 0.0315,\u001b[31m Korean,\u001b[0m 0.0293,\u001b[31m English and\u001b[0m 0.0212,\u001b[31m Chinese.\u001b[0m 0.0195,\u001b[31m.\n",
      "\u001b[0m 0.0189,\u001b[31m Korean and\u001b[0m 0.0162,\u001b[31m English,\u001b[0m 0.0156,\n",
      "input 11\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Mason Brooks is from speak \u001b[30mTarget: \u001b[0mItalian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0646,\u001b[31m a language\u001b[0m 0.0602,\u001b[31m English and\u001b[0m 0.0382,\u001b[31m a different\u001b[0m 0.0359,\u001b[31m English,\u001b[0m 0.0345,\u001b[31m with a\u001b[0m 0.0338,\u001b[31m Spanish.\u001b[0m 0.0300,\u001b[31m French.\u001b[0m 0.0292,\u001b[31m a dialect\u001b[0m 0.0217,\n",
      "input 12\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Lily Cooper is from speak \u001b[30mTarget: \u001b[0mEnglish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0755,\u001b[31m a language\u001b[0m 0.0714,\u001b[31m with a\u001b[0m 0.0467,\u001b[31m English,\u001b[0m 0.0405,\u001b[31m in a\u001b[0m 0.0295,\u001b[31m a different\u001b[0m 0.0271,\u001b[31m English and\u001b[0m 0.0265,\u001b[31m a dialect\u001b[0m 0.0241,\u001b[31m French.\u001b[0m 0.0185,\n",
      "input 13\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Jackson Bell is from speak \u001b[30mTarget: \u001b[0mSpanish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0664,\u001b[31m with a\u001b[0m 0.0482,\u001b[31m a different\u001b[0m 0.0320,\u001b[31m English.\u001b[0m 0.0290,\u001b[31m French.\u001b[0m 0.0267,\u001b[31m English,\u001b[0m 0.0258,\u001b[31m in a\u001b[0m 0.0239,\u001b[31m English and\u001b[0m 0.0214,\u001b[31m with an\u001b[0m 0.0207,\n",
      "input 14\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Amelia Ward is from speak \u001b[30mTarget: \u001b[0mGreek\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0618,\u001b[31m a language\u001b[0m 0.0471,\u001b[31m with a\u001b[0m 0.0437,\u001b[31m English and\u001b[0m 0.0394,\u001b[31m a dialect\u001b[0m 0.0361,\u001b[31m English,\u001b[0m 0.0355,\u001b[31m a different\u001b[0m 0.0312,\u001b[31m Amelia\u001b[0m 0.0283,\u001b[31m the Am\u001b[0m 0.0152,\n",
      "input 15\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Caleb Bryant is from speak \u001b[30mTarget: \u001b[0mVietnamese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0916,\u001b[31m English and\u001b[0m 0.0649,\u001b[31m French.\u001b[0m 0.0449,\u001b[31m a language\u001b[0m 0.0363,\u001b[31m English,\u001b[0m 0.0324,\u001b[31m Korean.\u001b[0m 0.0322,\u001b[31m Chinese.\u001b[0m 0.0321,\u001b[31m Vietnamese.\u001b[0m 0.0320,\u001b[31m a dialect\u001b[0m 0.0202,\n",
      "input 16\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Chloe Campbell is from speak \u001b[30mTarget: \u001b[0mAmharic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m French.\u001b[0m 0.1159,\u001b[31m English.\u001b[0m 0.0706,\u001b[31m Chinese.\u001b[0m 0.0437,\u001b[31m English and\u001b[0m 0.0411,\u001b[31m Spanish.\u001b[0m 0.0352,\u001b[31m English,\u001b[0m 0.0336,\u001b[31m French and\u001b[0m 0.0336,\u001b[31m Arabic.\u001b[0m 0.0220,\u001b[31m a language\u001b[0m 0.0210,\n",
      "input 17\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Henry Morgan is from speak \u001b[30mTarget: \u001b[0mIndonesian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.1338,\u001b[31m English and\u001b[0m 0.0389,\u001b[31m English.\u001b[0m 0.0384,\u001b[31m English,\u001b[0m 0.0366,\u001b[31m a different\u001b[0m 0.0333,\u001b[31m a dialect\u001b[0m 0.0219,\u001b[31m the same\u001b[0m 0.0126,\u001b[31m another language\u001b[0m 0.0120,\u001b[31m Hawaiian.\u001b[0m 0.0108,\n",
      "input 18\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ella Adams is from speak \u001b[30mTarget: \u001b[0mPersian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0903,\u001b[31m English and\u001b[0m 0.0514,\u001b[31m Persian.\u001b[0m 0.0399,\u001b[31m Arabic.\u001b[0m 0.0376,\u001b[31m English,\u001b[0m 0.0293,\u001b[31m Spanish.\u001b[0m 0.0228,\u001b[31m a language\u001b[0m 0.0200,\u001b[31m Korean.\u001b[0m 0.0168,\u001b[31m Russian.\u001b[0m 0.0159,\n",
      "input 19\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Owen Foster is from speak \u001b[30mTarget: \u001b[0mSwahili\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0982,\u001b[31m English and\u001b[0m 0.0528,\u001b[31m a language\u001b[0m 0.0508,\u001b[31m French.\u001b[0m 0.0483,\u001b[31m English,\u001b[0m 0.0388,\u001b[31m Spanish.\u001b[0m 0.0242,\u001b[31m a different\u001b[0m 0.0209,\u001b[31m with a\u001b[0m 0.0198,\u001b[31m French and\u001b[0m 0.0183,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80c5033e7b148588124dade0edffa6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'pad_token_id': 100277, 'eos_token_id': 100257}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached get_model_outputs_beam_search to file analysis/cache_dir/get_model_outputs_beam_search/978b5b295e4cf5e65e16e364fb386314fe431ae57b2c8bcc84629db9288eee11.pkl\n",
      "Experiment: olmo 2 not work----------------------------------------------------------------------------------------------------\n",
      "input 0\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Grace Miller is from speak \u001b[30mTarget: \u001b[0mJapanese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1192,\u001b[31m Spanish.\u001b[0m 0.0606,\u001b[31m English.\n",
      "\u001b[0m 0.0528,\u001b[31m French.\n",
      "\u001b[0m 0.0485,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0354,\u001b[31m a language\u001b[0m 0.0276,\u001b[31m German.\n",
      "\u001b[0m 0.0251,\u001b[31m French.\u001b[0m 0.0216,\u001b[31m English.\u001b[0m 0.0189,\n",
      "input 1\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ethan Parker is from speak \u001b[30mTarget: \u001b[0mMandarin\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.0946,\u001b[31m English.\n",
      "\u001b[0m 0.0580,\u001b[31m Spanish.\u001b[0m 0.0566,\u001b[31m French.\n",
      "\u001b[0m 0.0353,\u001b[31m English.\u001b[0m 0.0282,\u001b[31m a language\u001b[0m 0.0270,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0218,\u001b[31m French.\u001b[0m 0.0203,\u001b[31m English,\u001b[0m 0.0156,\n",
      "input 2\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Olivia Hughes is from speak \u001b[30mTarget: \u001b[0mMarathi\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1963,\u001b[31m Spanish.\u001b[0m 0.0737,\u001b[31m English.\n",
      "\u001b[0m 0.0673,\u001b[31m French.\n",
      "\u001b[0m 0.0515,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0328,\u001b[31m German.\n",
      "\u001b[0m 0.0228,\u001b[31m a language\u001b[0m 0.0199,\u001b[31m Portuguese.\n",
      "\u001b[0m 0.0193,\u001b[31m French.\u001b[0m 0.0187,\n",
      "input 3\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Jacob Turner is from speak \u001b[30mTarget: \u001b[0mFrench\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1247,\u001b[31m English.\n",
      "\u001b[0m 0.0738,\u001b[31m French.\n",
      "\u001b[0m 0.0531,\u001b[31m Spanish.\u001b[0m 0.0471,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0325,\u001b[31m German.\n",
      "\u001b[0m 0.0244,\u001b[31m French.\u001b[0m 0.0184,\u001b[31m what language\u001b[0m 0.0184,\u001b[31m English.\u001b[0m 0.0166,\n",
      "input 4\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ava Stewart is from speak \u001b[30mTarget: \u001b[0mGerman\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1470,\u001b[31m English.\n",
      "\u001b[0m 0.0612,\u001b[31m Spanish.\u001b[0m 0.0599,\u001b[31m French.\n",
      "\u001b[0m 0.0572,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0382,\u001b[31m what language\u001b[0m 0.0279,\u001b[31m French.\u001b[0m 0.0223,\u001b[31m a language\u001b[0m 0.0193,\u001b[31m German.\n",
      "\u001b[0m 0.0183,\n",
      "input 5\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Noah Clark is from speak \u001b[30mTarget: \u001b[0mRussian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1614,\u001b[31m English.\n",
      "\u001b[0m 0.0901,\u001b[31m French.\n",
      "\u001b[0m 0.0567,\u001b[31m Spanish.\u001b[0m 0.0527,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0292,\u001b[31m German.\n",
      "\u001b[0m 0.0264,\u001b[31m what language\u001b[0m 0.0216,\u001b[31m French.\u001b[0m 0.0188,\u001b[31m Arabic.\n",
      "\u001b[0m 0.0171,\n",
      "input 6\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Emma Howard is from speak \u001b[30mTarget: \u001b[0mArabic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1000,\u001b[31m English.\n",
      "\u001b[0m 0.0744,\u001b[31m French.\n",
      "\u001b[0m 0.0687,\u001b[31m Spanish.\u001b[0m 0.0383,\u001b[31m German.\n",
      "\u001b[0m 0.0328,\u001b[31m a language\u001b[0m 0.0283,\u001b[31m French.\u001b[0m 0.0271,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0223,\u001b[31m English.\u001b[0m 0.0184,\n",
      "input 7\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Liam Bennett is from speak \u001b[30mTarget: \u001b[0mThai\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1369,\u001b[31m English.\n",
      "\u001b[0m 0.0726,\u001b[31m French.\n",
      "\u001b[0m 0.0553,\u001b[31m Spanish.\u001b[0m 0.0448,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0250,\u001b[31m a language\u001b[0m 0.0237,\u001b[31m German.\n",
      "\u001b[0m 0.0229,\u001b[31m what language\u001b[0m 0.0196,\u001b[31m French.\u001b[0m 0.0186,\n",
      "input 8\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mia Sanders is from speak \u001b[30mTarget: \u001b[0mTurkish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1190,\u001b[31m Spanish.\u001b[0m 0.0692,\u001b[31m English.\n",
      "\u001b[0m 0.0612,\u001b[31m French.\n",
      "\u001b[0m 0.0440,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0286,\u001b[31m a language\u001b[0m 0.0266,\u001b[31m French.\u001b[0m 0.0256,\u001b[31m English.\u001b[0m 0.0249,\u001b[31m German.\n",
      "\u001b[0m 0.0222,\n",
      "input 9\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Lucas Foster is from speak \u001b[30mTarget: \u001b[0mPortuguese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0773,\u001b[31m Spanish.\u001b[0m 0.0426,\u001b[31m a different\u001b[0m 0.0420,\u001b[31m Spanish.\n",
      "\u001b[0m 0.0339,\u001b[31m the same\u001b[0m 0.0233,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0179,\u001b[31m English,\u001b[0m 0.0170,\u001b[31m Spanish,\u001b[0m 0.0157,\u001b[31m French.\u001b[0m 0.0145,\n",
      "input 10\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Sophia Hayes is from speak \u001b[30mTarget: \u001b[0mKorean\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.2161,\u001b[31m Spanish.\u001b[0m 0.0763,\u001b[31m English.\n",
      "\u001b[0m 0.0643,\u001b[31m French.\n",
      "\u001b[0m 0.0492,\u001b[31m German.\n",
      "\u001b[0m 0.0329,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0299,\u001b[31m a language\u001b[0m 0.0205,\u001b[31m French.\u001b[0m 0.0170,\u001b[31m Portuguese.\n",
      "\u001b[0m 0.0139,\n",
      "input 11\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mason Brooks is from speak \u001b[30mTarget: \u001b[0mItalian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0588,\u001b[31m a different\u001b[0m 0.0559,\u001b[31m Spanish.\u001b[0m 0.0340,\u001b[31m the same\u001b[0m 0.0251,\u001b[31m English,\u001b[0m 0.0230,\u001b[31m with a\u001b[0m 0.0223,\u001b[31m English.\u001b[0m 0.0219,\u001b[31m with an\u001b[0m 0.0219,\u001b[31m Spanish.\n",
      "\u001b[0m 0.0211,\n",
      "input 12\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Lily Cooper is from speak \u001b[30mTarget: \u001b[0mEnglish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.1726,\u001b[31m French.\n",
      "\u001b[0m 0.0871,\u001b[31m English.\n",
      "\u001b[0m 0.0641,\u001b[31m Spanish.\u001b[0m 0.0636,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0330,\u001b[31m French.\u001b[0m 0.0318,\u001b[31m German.\n",
      "\u001b[0m 0.0306,\u001b[31m a language\u001b[0m 0.0186,\u001b[31m Italian.\n",
      "\u001b[0m 0.0180,\n",
      "input 13\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Jackson Bell is from speak \u001b[30mTarget: \u001b[0mSpanish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.0967,\u001b[31m Spanish.\u001b[0m 0.0711,\u001b[31m English.\n",
      "\u001b[0m 0.0669,\u001b[31m French.\n",
      "\u001b[0m 0.0349,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0322,\u001b[31m English.\u001b[0m 0.0318,\u001b[31m a language\u001b[0m 0.0302,\u001b[31m French.\u001b[0m 0.0266,\u001b[31m English,\u001b[0m 0.0176,\n",
      "input 14\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Amelia Ward is from speak \u001b[30mTarget: \u001b[0mGreek\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.0880,\u001b[31m English.\n",
      "\u001b[0m 0.0754,\u001b[31m Spanish.\u001b[0m 0.0455,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0355,\u001b[31m French.\n",
      "\u001b[0m 0.0349,\u001b[31m what language\u001b[0m 0.0287,\u001b[31m English.\u001b[0m 0.0256,\u001b[31m a language\u001b[0m 0.0215,\u001b[31m English.\n",
      "\n",
      "\u001b[0m 0.0198,\n",
      "input 15\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Caleb Bryant is from speak \u001b[30mTarget: \u001b[0mVietnamese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m what language\u001b[0m 0.0684,\u001b[31m a language\u001b[0m 0.0638,\u001b[31m English.\u001b[0m 0.0504,\u001b[31m English.\n",
      "\u001b[0m 0.0434,\u001b[31m a different\u001b[0m 0.0380,\u001b[31m which language\u001b[0m 0.0375,\u001b[31m English,\u001b[0m 0.0375,\u001b[31m English.\n",
      "\n",
      "\u001b[0m 0.0248,\u001b[31m the same\u001b[0m 0.0241,\n",
      "input 16\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Chloe Campbell is from speak \u001b[30mTarget: \u001b[0mAmharic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0516,\u001b[31m English.\n",
      "\u001b[0m 0.0482,\u001b[31m what language\u001b[0m 0.0389,\u001b[31m English,\u001b[0m 0.0377,\u001b[31m Spanish.\n",
      "\u001b[0m 0.0368,\u001b[31m Spanish.\u001b[0m 0.0357,\u001b[31m a language\u001b[0m 0.0329,\u001b[31m French.\u001b[0m 0.0289,\u001b[31m French.\n",
      "\u001b[0m 0.0265,\n",
      "input 17\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Henry Morgan is from speak \u001b[30mTarget: \u001b[0mIndonesian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\u001b[0m 0.0686,\u001b[31m English.\u001b[0m 0.0642,\u001b[31m English.\n",
      "\u001b[0m 0.0506,\u001b[31m English,\u001b[0m 0.0405,\u001b[31m Spanish.\n",
      "\u001b[0m 0.0392,\u001b[31m a language\u001b[0m 0.0353,\u001b[31m a different\u001b[0m 0.0348,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0288,\u001b[31m Spanish,\u001b[0m 0.0255,\n",
      "input 18\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ella Adams is from speak \u001b[30mTarget: \u001b[0mPersian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.0660,\u001b[31m Spanish.\u001b[0m 0.0360,\u001b[31m English.\n",
      "\u001b[0m 0.0357,\u001b[31m French.\n",
      "\u001b[0m 0.0344,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0277,\u001b[31m what language\u001b[0m 0.0260,\u001b[31m a language\u001b[0m 0.0238,\u001b[31m German.\n",
      "\u001b[0m 0.0216,\u001b[31m French.\u001b[0m 0.0165,\n",
      "input 19\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Owen Foster is from speak \u001b[30mTarget: \u001b[0mSwahili\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m Spanish.\n",
      "\u001b[0m 0.0876,\u001b[31m English.\n",
      "\u001b[0m 0.0816,\u001b[31m French.\n",
      "\u001b[0m 0.0471,\u001b[31m what language\u001b[0m 0.0390,\u001b[31m Spanish.\u001b[0m 0.0365,\u001b[31m a language\u001b[0m 0.0330,\u001b[31m Spanish.\n",
      "\n",
      "\u001b[0m 0.0261,\u001b[31m English.\u001b[0m 0.0247,\u001b[31m a different\u001b[0m 0.0205,\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6deba5297d6414dba44cdf459e5b0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached get_model_outputs_beam_search to file analysis/cache_dir/get_model_outputs_beam_search/f5ae856559453a6b7570006810e27b68b7ccecd50d01b1d39ed27df885dd5e68.pkl\n",
      "Experiment: olmo 1 Kinda Work----------------------------------------------------------------------------------------------------\n",
      "input 0\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Grace Miller is from speak \u001b[30mTarget: \u001b[0mJapanese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0975,\u001b[31m a language\u001b[0m 0.0539,\u001b[31m English,\u001b[0m 0.0377,\u001b[31m a different\u001b[0m 0.0331,\u001b[31m English and\u001b[0m 0.0316,\u001b[31m French.\u001b[0m 0.0245,\u001b[31m Spanish.\u001b[0m 0.0213,\u001b[31m what language\u001b[0m 0.0171,\u001b[31m a dialect\u001b[0m 0.0156,\n",
      "input 1\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Ethan Parker is from speak \u001b[30mTarget: \u001b[0mMandarin\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0874,\u001b[31m a language\u001b[0m 0.0483,\u001b[31m English,\u001b[0m 0.0357,\u001b[31m what language\u001b[0m 0.0207,\u001b[31m with a\u001b[0m 0.0204,\u001b[31m a different\u001b[0m 0.0197,\u001b[31m French.\u001b[0m 0.0191,\u001b[31m a dialect\u001b[0m 0.0186,\u001b[31m English and\u001b[0m 0.0185,\n",
      "input 2\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Olivia Hughes is from speak \u001b[30mTarget: \u001b[0mMarathi\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0810,\u001b[31m a language\u001b[0m 0.0548,\u001b[31m French.\u001b[0m 0.0352,\u001b[31m English,\u001b[0m 0.0299,\u001b[31m Spanish.\u001b[0m 0.0282,\u001b[31m a different\u001b[0m 0.0279,\u001b[31m English and\u001b[0m 0.0247,\u001b[31m what language\u001b[0m 0.0135,\u001b[31m a dialect\u001b[0m 0.0134,\n",
      "input 3\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Jacob Turner is from speak \u001b[30mTarget: \u001b[0mFrench\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0740,\u001b[31m English.\u001b[0m 0.0430,\u001b[31m a dialect\u001b[0m 0.0326,\u001b[31m French.\u001b[0m 0.0283,\u001b[31m a different\u001b[0m 0.0277,\u001b[31m English,\u001b[0m 0.0252,\u001b[31m with a\u001b[0m 0.0243,\u001b[31m the same\u001b[0m 0.0174,\u001b[31m French,\u001b[0m 0.0155,\n",
      "input 4\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ava Stewart is from speak \u001b[30mTarget: \u001b[0mGerman\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1062,\u001b[31m English and\u001b[0m 0.0479,\u001b[31m French.\u001b[0m 0.0393,\u001b[31m English,\u001b[0m 0.0335,\u001b[31m Spanish.\u001b[0m 0.0312,\u001b[31m a language\u001b[0m 0.0200,\u001b[31m French and\u001b[0m 0.0182,\u001b[31m French,\u001b[0m 0.0166,\u001b[31m Arabic.\u001b[0m 0.0142,\n",
      "input 5\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Noah Clark is from speak \u001b[30mTarget: \u001b[0mRussian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1021,\u001b[31m a language\u001b[0m 0.0723,\u001b[31m English,\u001b[0m 0.0377,\u001b[31m English and\u001b[0m 0.0259,\u001b[31m French.\u001b[0m 0.0202,\u001b[31m a dialect\u001b[0m 0.0170,\u001b[31m a different\u001b[0m 0.0155,\u001b[31m the same\u001b[0m 0.0151,\u001b[31m what language\u001b[0m 0.0147,\n",
      "input 6\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Emma Howard is from speak \u001b[30mTarget: \u001b[0mArabic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0821,\u001b[31m a language\u001b[0m 0.0484,\u001b[31m English,\u001b[0m 0.0337,\u001b[31m a dialect\u001b[0m 0.0277,\u001b[31m with a\u001b[0m 0.0249,\u001b[31m a different\u001b[0m 0.0239,\u001b[31m English and\u001b[0m 0.0197,\u001b[31m what language\u001b[0m 0.0160,\u001b[31m in a\u001b[0m 0.0121,\n",
      "input 7\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Liam Bennett is from speak \u001b[30mTarget: \u001b[0mThai\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0838,\u001b[31m a language\u001b[0m 0.0575,\u001b[31m English,\u001b[0m 0.0384,\u001b[31m a different\u001b[0m 0.0245,\u001b[31m English and\u001b[0m 0.0219,\u001b[31m a dialect\u001b[0m 0.0195,\u001b[31m with a\u001b[0m 0.0183,\u001b[31m French.\u001b[0m 0.0154,\u001b[31m the same\u001b[0m 0.0133,\n",
      "input 8\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Mia Sanders is from speak \u001b[30mTarget: \u001b[0mTurkish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0759,\u001b[31m German.\u001b[0m 0.0393,\u001b[31m a language\u001b[0m 0.0337,\u001b[31m Spanish.\u001b[0m 0.0318,\u001b[31m what language\u001b[0m 0.0312,\u001b[31m English,\u001b[0m 0.0239,\u001b[31m a different\u001b[0m 0.0238,\u001b[31m English and\u001b[0m 0.0224,\u001b[31m French.\u001b[0m 0.0224,\n",
      "input 9\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Lucas Foster is from speak \u001b[30mTarget: \u001b[0mPortuguese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0820,\u001b[31m Spanish.\u001b[0m 0.0516,\u001b[31m a language\u001b[0m 0.0440,\u001b[31m English,\u001b[0m 0.0432,\u001b[31m English and\u001b[0m 0.0232,\u001b[31m Spanish,\u001b[0m 0.0230,\u001b[31m a different\u001b[0m 0.0215,\u001b[31m with a\u001b[0m 0.0161,\u001b[31m Spanish and\u001b[0m 0.0138,\n",
      "input 10\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Sophia Hayes is from speak \u001b[30mTarget: \u001b[0mKorean\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0856,\u001b[31m a language\u001b[0m 0.0590,\u001b[31m English and\u001b[0m 0.0286,\u001b[31m English,\u001b[0m 0.0273,\u001b[31m what language\u001b[0m 0.0211,\u001b[31m Greek.\u001b[0m 0.0189,\u001b[31m French.\u001b[0m 0.0182,\u001b[31m Russian.\u001b[0m 0.0171,\u001b[31m a different\u001b[0m 0.0168,\n",
      "input 11\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Mason Brooks is from speak \u001b[30mTarget: \u001b[0mItalian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0946,\u001b[31m a language\u001b[0m 0.0760,\u001b[31m English,\u001b[0m 0.0361,\u001b[31m a different\u001b[0m 0.0276,\u001b[31m English and\u001b[0m 0.0243,\u001b[31m French.\u001b[0m 0.0204,\u001b[31m a dialect\u001b[0m 0.0183,\u001b[31m Spanish.\u001b[0m 0.0156,\u001b[31m the same\u001b[0m 0.0145,\n",
      "input 12\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Lily Cooper is from speak \u001b[30mTarget: \u001b[0mEnglish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m with a\u001b[0m 0.1035,\u001b[31m British English\u001b[0m 0.0428,\u001b[31m Cockney\u001b[0m 0.0401,\u001b[31m English.\u001b[0m 0.0354,\u001b[31m with an\u001b[0m 0.0298,\u001b[31m in a\u001b[0m 0.0235,\u001b[31m with British\u001b[0m 0.0218,\u001b[31m American English\u001b[0m 0.0187,\u001b[31m English,\u001b[0m 0.0179,\n",
      "input 13\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Jackson Bell is from speak \u001b[30mTarget: \u001b[0mSpanish\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.0684,\u001b[31m English.\u001b[0m 0.0668,\u001b[31m Spanish.\u001b[0m 0.0592,\u001b[31m English,\u001b[0m 0.0258,\u001b[31m a different\u001b[0m 0.0227,\u001b[31m Spanish,\u001b[0m 0.0216,\u001b[31m French.\u001b[0m 0.0190,\u001b[31m English and\u001b[0m 0.0166,\u001b[31m the same\u001b[0m 0.0154,\n",
      "input 14\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Amelia Ward is from speak \u001b[30mTarget: \u001b[0mGreek\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0939,\u001b[31m a language\u001b[0m 0.0617,\u001b[31m English and\u001b[0m 0.0397,\u001b[31m English,\u001b[0m 0.0359,\u001b[31m a different\u001b[0m 0.0249,\u001b[31m a dialect\u001b[0m 0.0235,\u001b[31m what language\u001b[0m 0.0206,\u001b[31m French.\u001b[0m 0.0177,\u001b[31m Spanish.\u001b[0m 0.0161,\n",
      "input 15\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Caleb Bryant is from speak \u001b[30mTarget: \u001b[0mVietnamese\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.0920,\u001b[31m a language\u001b[0m 0.0470,\u001b[31m Spanish.\u001b[0m 0.0410,\u001b[31m English and\u001b[0m 0.0396,\u001b[31m English,\u001b[0m 0.0271,\u001b[31m French.\u001b[0m 0.0239,\u001b[31m a different\u001b[0m 0.0165,\u001b[31m Spanish and\u001b[0m 0.0164,\u001b[31m a dialect\u001b[0m 0.0155,\n",
      "input 16\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Chloe Campbell is from speak \u001b[30mTarget: \u001b[0mAmharic\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1081,\u001b[31m French.\u001b[0m 0.0577,\u001b[31m a language\u001b[0m 0.0399,\u001b[31m English and\u001b[0m 0.0368,\u001b[31m English,\u001b[0m 0.0305,\u001b[31m Spanish.\u001b[0m 0.0244,\u001b[31m French and\u001b[0m 0.0219,\u001b[31m Arabic.\u001b[0m 0.0181,\u001b[31m French,\u001b[0m 0.0180,\n",
      "input 17\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Henry Morgan is from speak \u001b[30mTarget: \u001b[0mIndonesian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m a language\u001b[0m 0.1215,\u001b[31m Spanish.\u001b[0m 0.0400,\u001b[31m English.\u001b[0m 0.0391,\u001b[31m a different\u001b[0m 0.0260,\u001b[31m what language\u001b[0m 0.0245,\u001b[31m English,\u001b[0m 0.0219,\u001b[31m a dialect\u001b[0m 0.0200,\u001b[31m Portuguese.\u001b[0m 0.0176,\u001b[31m Spanish,\u001b[0m 0.0154,\n",
      "input 18\n",
      "\u001b[30mInput: \u001b[0mThe people in the city Ella Adams is from speak \u001b[30mTarget: \u001b[0mPersian\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1102,\u001b[31m a language\u001b[0m 0.0419,\u001b[31m English and\u001b[0m 0.0409,\u001b[31m English,\u001b[0m 0.0310,\u001b[31m what language\u001b[0m 0.0260,\u001b[31m French.\u001b[0m 0.0240,\u001b[31m German.\u001b[0m 0.0174,\u001b[31m Arabic.\u001b[0m 0.0152,\u001b[31m a different\u001b[0m 0.0137,\n",
      "input 19\n",
      "\u001b[30mInput: \u001b[0m<|padding|>The people in the city Owen Foster is from speak \u001b[30mTarget: \u001b[0mSwahili\n",
      "\u001b[30mOutputs: \u001b[0m\u001b[31m English.\u001b[0m 0.1105,\u001b[31m a language\u001b[0m 0.0523,\u001b[31m English,\u001b[0m 0.0342,\u001b[31m English and\u001b[0m 0.0270,\u001b[31m a different\u001b[0m 0.0259,\u001b[31m a dialect\u001b[0m 0.0236,\u001b[31m French.\u001b[0m 0.0201,\u001b[31m with a\u001b[0m 0.0199,\u001b[31m what language\u001b[0m 0.0195,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class HighLogProbabilityDatapoint:\n",
    "    path: Path\n",
    "    checkpoint_name: str\n",
    "    influence_analysis_path: Path | None = None\n",
    "    experiment_name: str = \"\"\n",
    "    \n",
    "    num_outputs_to_visualize: int = 20\n",
    "    \n",
    "    num_beams: int = 12\n",
    "    num_return_sequences: int = 9\n",
    "    max_new_tokens: int = 2\n",
    "    num_inputs: int = 10\n",
    "\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "experiments_to_analyze = [\n",
    "#     HighLogProbabilityDatapoint(path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_04_30_21-12-37_c60_lr_sweep_olmo2/2025_05_01_21-36-20_376_olmo_2_rerun_first_hop_num_facts_20_num_epochs_None_lr_0.0001\"), checkpoint_name=\"checkpoint_final\",experiment_name=\"Olmo 2\"),      \n",
    "]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available()\n",
    "import json\n",
    "run_ids = [\n",
    "    (\"3urxrbpg\", \"olmo 1 working\"),\n",
    "    (\"y0ssjv88\", \"olmo 2 not work\"),\n",
    "    (\"iyqvvoeo\", \"olmo 1 Kinda Work\"),\n",
    "]\n",
    "\n",
    "for run_id, run_name in run_ids:\n",
    "    experiment_args, output_dir = run_id_to_training_args(run_id,args_clss=TrainingArgs)\n",
    "    experiments_to_analyze += [HighLogProbabilityDatapoint(path=Path(output_dir), checkpoint_name=\"checkpoint_final\",experiment_name=run_name)]\n",
    "\n",
    "    \n",
    "for experiment in experiments_to_analyze:\n",
    "    log_state =  LogState.model_validate_json(Path(experiment.path / \"experiment_log.json\").read_text())\n",
    "    args = {k:v for k,v in log_state.args.items() if k in TrainingArgs.model_json_schema()[\"properties\"]} # type: ignore\n",
    "    args = TrainingArgs.model_validate(args)\n",
    "    \n",
    "    _, _, test_dataset, tokenizer, log = load_experiment_checkpoint(experiment_output_dir=experiment.path, checkpoint_name=experiment.checkpoint_name, load_model=False, load_tokenizer=True)\n",
    "    test_dataset = test_dataset[\"inferred_facts\"]\n",
    "    model_inputs = test_dataset[\"input_ids\"][:experiment.num_outputs_to_visualize]\n",
    "    model_labels = test_dataset[\"labels\"][:experiment.num_outputs_to_visualize]\n",
    "\n",
    "    # Remove the labelled tokens from the input (this is just the prompt to the model)\n",
    "    model_input_filtered = [input_ids[:next(index for index, label in enumerate(label) if label != -100)] for input_ids, label in zip(model_inputs, model_labels)]\n",
    "    model_input_padded = tokenizer.pad({\"input_ids\": model_input_filtered}, padding_side=\"left\",return_tensors=\"pt\").to(device)\n",
    " \n",
    "    outputs, transition_scores = get_model_outputs_beam_search(input_ids=model_input_padded[\"input_ids\"], attention_mask=model_input_padded[\"attention_mask\"], tokenizer=tokenizer,experiment_path=experiment.path, checkpoint_name=experiment.checkpoint_name, max_new_tokens=experiment.max_new_tokens, num_beams=experiment.num_beams, num_return_sequences=experiment.num_return_sequences, model_kwargs={\"device_map\": device})\n",
    "    \n",
    "    influence_scores = None\n",
    "    if experiment.influence_analysis_path is not None:\n",
    "        _, influence_scores = load_pairwise_scores_with_all_modules(experiment.influence_analysis_path)\n",
    "    \n",
    "\n",
    "    print(f\"Experiment: {experiment.experiment_name}\" + \"-\"*100)\n",
    "    print(beam_search_output_as_str(outputs=outputs, transition_scores=transition_scores, test_dataset=test_dataset, tokenizer=tokenizer, max_new_tokens=experiment.max_new_tokens, num_return_sequences=experiment.num_return_sequences, split_per_token_probs=False, influence_scores=influence_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
