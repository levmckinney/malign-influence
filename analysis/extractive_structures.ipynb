{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of extractive structures results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Add the top level of the directory to the python path, so we can import the scripts\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from shared_ml.utils import get_root_of_git_repo\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers.generation.utils import GenerationConfig, GenerateBeamDecoderOnlyOutput\n",
    "from shared_ml.utils import default_function_args_to_cache_id, hash_str\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from typing import Any\n",
    "from termcolor import colored\n",
    "repo_root = get_root_of_git_repo()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, cast\n",
    "import seaborn as sns\n",
    "import json\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "from oocr_influence.cli.run_influence import InfluenceArgs\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "from oocr_influence.cli.run_influence import InfluenceArgs\n",
    "from dataclasses import dataclass\n",
    "from datasets import DatasetDict\n",
    "from shared_ml.logging import LogState, load_log_from_wandb, paths_or_wandb_to_logs\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import TypeVar\n",
    "from numpy.typing import NDArray\n",
    "from shared_ml.utils import cache_function_outputs\n",
    "from itertools import chain, groupby\n",
    "import numpy.typing as npt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "# Also chang the CWD to the repo, so we can import items from the various scripts.\n",
    "os.chdir(repo_root)\n",
    "from shared_ml.logging import load_experiment_checkpoint\n",
    "\n",
    "# from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier, add_box_to_mnist_dataset\n",
    "\n",
    "import logging\n",
    "from typing import Literal\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, Conv1D\n",
    "from torch import nn\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from datasets import Dataset\n",
    "from kronfluence.arguments import FactorArguments, ScoreArguments\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.utils.common.factor_arguments import all_low_precision_factor_arguments\n",
    "from kronfluence.utils.common.score_arguments import all_low_precision_score_arguments\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "import numpy as np\n",
    "from kronfluence.score import load_pairwise_scores\n",
    "from oocr_influence.cli.run_influence import InfluenceArgs\n",
    "\n",
    "# from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier, add_box_to_mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "(datetime.now() - datetime(datetime.now().year, 1, 1)).days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "from typing import Any, Literal\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "from functools import cache\n",
    "from typing import Generator\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from shared_ml.utils import cache_function_outputs\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "import seaborn as sns\n",
    "from termcolor import colored\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache_id = lambda x: hashlib.sha256(x[\"input_array\"].tobytes()).hexdigest()[:8]) # type: ignore\n",
    "def rank_influence_scores(input_array: np.ndarray[Any, Any] | torch.Tensor) -> np.ndarray[Any, Any]:\n",
    "    if isinstance(input_array, torch.Tensor):\n",
    "        input_array = input_array.cpu().numpy()\n",
    "    return np.argsort(np.argsort(-input_array, axis=1), axis=1)\n",
    "\n",
    "def get_parent_influence_scores(influence_scores: np.ndarray[Any,np.dtype[Any]] | torch.Tensor, test_dataset: Dataset) -> np.ndarray[Any,np.dtype[Any]]:\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores = influence_scores.cpu().numpy()\n",
    "    parent_idxs: list[int] = test_dataset[\"parent_fact_idx\"]\n",
    "    influence_scores_by_parent = influence_scores[np.arange(len(influence_scores)), parent_idxs]\n",
    "    return influence_scores_by_parent\n",
    "\n",
    "\n",
    "def cache_get_parent_influence_ranks(args: dict[str,Any]) -> str:\n",
    "    train_dataset_fingerprint = args[\"train_dataset\"]._fingerprint\n",
    "    test_dataset_fingerprint = args[\"test_dataset\"]._fingerprint\n",
    "    influence_scores_hash = hashlib.sha256(args[\"influence_scores\"].tobytes()).hexdigest()[:8]\n",
    "    non_parents_instead_of_parents = str(args[\"non_parents_instead_of_parents\"])\n",
    "    return hash_str(f\"{train_dataset_fingerprint}-{test_dataset_fingerprint}-{influence_scores_hash}-{non_parents_instead_of_parents}\")\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache_id = cache_get_parent_influence_ranks)\n",
    "def get_parent_influence_ranks(influence_scores: NDArray[Any] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, non_parents_instead_of_parents : bool = True) -> dict[int,NDArray[Any]]:\n",
    "    \n",
    "    influence_scores_rank = rank_influence_scores(influence_scores)\n",
    "\n",
    "    types = train_dataset[\"type\"]\n",
    "    idxs = train_dataset[\"idx\"]\n",
    "    train_set_parent_idxs = [idx if \"fact\" in t else None for t,idx in zip(types,idxs)]\n",
    "    \n",
    "    parent_idxs_to_train_set_idxs = defaultdict(list)\n",
    "\n",
    "    for train_set_idx, train_set_parent_idx in enumerate(train_set_parent_idxs):\n",
    "        if train_set_parent_idx is not None:\n",
    "            parent_idxs_to_train_set_idxs[train_set_parent_idx].append(train_set_idx)\n",
    "        \n",
    "    if non_parents_instead_of_parents:\n",
    "        # Make it so that you are in the list if you are NOT a parent\n",
    "        all_indices = set(range(len(train_dataset)))\n",
    "        parent_idxs_to_train_set_idxs = {k: list(all_indices - set(v)) for k, v in parent_idxs_to_train_set_idxs.items()}\n",
    "    parent_idxs_to_influence_ranks = {parent_idx: influence_scores_rank[parent_idx,train_set_idxs] for parent_idx,train_set_idxs in parent_idxs_to_train_set_idxs.items()}\n",
    "\n",
    "    return parent_idxs_to_influence_ranks  \n",
    "\n",
    "def plot_histogram_train_subset(influence_scores: NDArray[Any] | torch.Tensor, train_dataset: Dataset, subset_inds: list[int], title: str, xlabel: str, ylabel: str,bin_width: int = 10,max_value: int | None = None, fig: Figure | None = None, ax: Axes | None = None):\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "\n",
    "    influence_ranks = rank_influence_scores(influence_scores)\n",
    "    max_value = max_value or np.max(influence_ranks)\n",
    "    subset_influence_ranks = influence_ranks[:,subset_inds]\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = fig.gca()\n",
    "    \n",
    "    ax.hist(subset_influence_ranks.flatten(), edgecolor=\"black\", bins=np.arange(0, max_value + 1, bin_width)) # type: ignore\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # show the figure\n",
    "    fig.show()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_histogram_parent_ranks(influence_scores: NDArray[Any] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, max_value: int | None = None, bin_width: int = 10,non_parents_instead_of_parents: bool = \n",
    "                                False,parent_inds: list[int] | None = None):\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    parent_influence_ranks = get_parent_influence_ranks(influence_scores, train_dataset, test_dataset,non_parents_instead_of_parents)\n",
    "    if parent_inds is not None:\n",
    "        parent_influence_ranks = {k: v for k, v in parent_influence_ranks.items() if k in parent_inds}\n",
    "    parent_influence_ranks = np.array(list(chain(*parent_influence_ranks.values())))\n",
    "    fig, ax = plt.subplots()    \n",
    "    max_value = max_value or np.max(parent_influence_ranks)\n",
    "    ax.hist(parent_influence_ranks.flatten(), edgecolor=\"black\", bins=np.arange(0, max_value + 1, bin_width))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    fig.show()\n",
    "\n",
    "def plot_histogram_parent_ranks_seaborn(\n",
    "    influence_scores: NDArray[Any] | torch.Tensor,\n",
    "    train_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    max_value: int | None = None,\n",
    "    bin_width: int = 10,\n",
    "    non_parents_instead_of_parents: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots overlaid histograms for parent influence ranks using Seaborn.\n",
    "\n",
    "    Each row in the calculated parent_influence_ranks array gets its own\n",
    "    histogram overlaid on the same plot.\n",
    "\n",
    "    Args:\n",
    "        influence_scores: A 2D array or tensor of influence scores (e.g., test_instances x train_instances).\n",
    "        train_dataset: The training dataset object.\n",
    "        test_dataset: The test dataset object.\n",
    "        title: The title for the plot.\n",
    "        xlabel: The label for the x-axis.\n",
    "        ylabel: The label for the y-axis.\n",
    "        max_value: The maximum value for the x-axis and bin calculation. If None, determined from data.\n",
    "        bin_width: The width of each histogram bin.\n",
    "        non_parents_instead_of_parents: Flag passed to get_parent_influence_ranks.\n",
    "    \"\"\"\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores_np = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    else:\n",
    "        influence_scores_np = np.asarray(influence_scores) # Ensure it's a numpy array\n",
    "\n",
    "    # Get the 2D array of ranks (DO NOT FLATTEN here)\n",
    "    parent_influence_ranks_2d = get_parent_influence_ranks(\n",
    "        influence_scores_np, train_dataset, test_dataset, non_parents_instead_of_parents\n",
    "    )\n",
    "\n",
    "    if parent_influence_ranks_2d.size == 0: # type: ignore\n",
    "        print(\"Warning: parent_influence_ranks_2d is empty. Cannot plot histogram.\")\n",
    "        return\n",
    "\n",
    "    # --- Convert data to long-form DataFrame for Seaborn ---\n",
    "    data_for_df = []\n",
    "    for i, ranks_for_row in enumerate(parent_influence_ranks_2d):\n",
    "        for rank in ranks_for_row:\n",
    "            data_for_df.append({'rank': rank, 'row_index': i})\n",
    "            \n",
    "    if not data_for_df:\n",
    "         print(\"Warning: No data found after processing ranks. Cannot plot histogram.\")\n",
    "         return\n",
    "         \n",
    "    df = pd.DataFrame(data_for_df)\n",
    "    # Ensure row_index is treated as a category for distinct colors\n",
    "    df['row_index'] = df['row_index'].astype('category') \n",
    "    # --- ---\n",
    "\n",
    "    # Determine the maximum value for bins if not provided\n",
    "    actual_max_rank = df['rank'].max()\n",
    "    plot_max_value = max_value if max_value is not None else actual_max_rank\n",
    "    \n",
    "    # Define bins carefully to include the max value\n",
    "    bins = np.arange(0, plot_max_value + bin_width, bin_width)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6)) # Adjust figsize as needed\n",
    "\n",
    "    # Use seaborn histplot\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x='rank',\n",
    "        hue='row_index', # Color histograms by original row index\n",
    "        bins=bins,\n",
    "        binwidth=bin_width if bins is None else None, # Use either bins or binwidth\n",
    "        element=\"step\",  # Use 'step' for better visibility of overlays\n",
    "        # kde=True,       # Uncomment to add Kernel Density Estimate plots\n",
    "        ax=ax,\n",
    "        palette='viridis', # Choose a color palette (optional)\n",
    "        legend=True        # Show legend\n",
    "    )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel) # Seaborn might label this 'Count', override if needed\n",
    "\n",
    "    # Optional: Set x-axis limit if max_value was specified\n",
    "    if max_value is not None:\n",
    "        ax.set_xlim(0, max_value)\n",
    "    else:\n",
    "        # Ensure the last bin edge is slightly beyond the max rank if automatically determined\n",
    "         ax.set_xlim(0, plot_max_value + bin_width) \n",
    "\n",
    "\n",
    "    # Improve legend if there are many rows\n",
    "    if len(df['row_index'].unique()) > 10:\n",
    "         ax.legend(title='Row Index', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "         plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    else:\n",
    "         ax.legend(title='Row Index')\n",
    "         plt.tight_layout()\n",
    "\n",
    "\n",
    "    # Use plt.show() for standard execution environments \n",
    "    # or fig.show() potentially in specific interactive environments\n",
    "    plt.show()\n",
    "\n",
    "def get_mlp_and_attention_groups(module_names: list[str]) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \n",
    "    layer_match = re.compile(r\"\\.(\\d+)\\.\")\n",
    "    layers = [int(layer_match.search(module_name).group(1)) for module_name in module_names ]\n",
    "    layer_mlp_groups : list[list[str]] = [[] for _ in range(max(layers)+ 1)]\n",
    "    layer_attention_groups : list[list[str]] = [[] for _ in range(max(layers)+ 1)]\n",
    "    \n",
    "    for module_name, layer in zip(module_names, layers):\n",
    "        if \"mlp\" in module_name:\n",
    "            layer_mlp_groups[layer].append(module_name)\n",
    "        elif \"attn\" in module_name:\n",
    "            layer_attention_groups[layer].append(module_name)\n",
    "\n",
    "    return layer_mlp_groups, layer_attention_groups\n",
    "def plot_heatmap_influence_scores_by_layer(influence_scores_by_layer: dict[str, np.ndarray] | dict[str, torch.Tensor], train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, aggregation_type: Literal[\"sum\", \"abs_sum\",\"ranks_above_median\",\"ranks_below_median\"] = \"sum\"):\n",
    "    if isinstance(next(iter(influence_scores_by_layer.values())), torch.Tensor):\n",
    "        influence_scores_by_layer = {k: v.to(dtype=torch.float32).cpu().numpy() for k, v in influence_scores_by_layer.items()} # type: ignore\n",
    "    parent_idxs = test_dataset[\"parent_fact_idx\"]\n",
    "    \n",
    "    groups_mlp, groups_attention = get_mlp_and_attention_groups(list(influence_scores_by_layer.keys()))\n",
    "     \n",
    "    title = f\"{title} ({aggregation_type})\"\n",
    "        \n",
    "    \n",
    "    if aggregation_type == \"sum\" or aggregation_type == \"abs_sum\":\n",
    "        groups_to_influence = {}\n",
    "        for group_name, group in zip([\"attention\", \"mlp\"], [groups_attention, groups_mlp]):\n",
    "            layer_group_to_influence = defaultdict(float)\n",
    "            for layer_num, layer_group in enumerate(group):\n",
    "                layer_group_influence = 0\n",
    "                for layer_name in layer_group:\n",
    "                    influence_score = influence_scores_by_layer[layer_name]\n",
    "                    influence_score_by_parent = influence_score[np.arange(len(influence_score)), parent_idxs]\n",
    "                    if aggregation_type == \"abs_sum\":\n",
    "                        influence_score_by_parent = np.abs(influence_score_by_parent)\n",
    "                    \n",
    "                    layer_group_influence += np.sum(influence_score_by_parent)\n",
    "                \n",
    "                layer_group_to_influence[f\"{group_name}_{layer_num}\"] = layer_group_influence\n",
    "            \n",
    "            groups_to_influence[group_name] = layer_group_to_influence\n",
    "    elif aggregation_type == \"ranks_below_median\" or aggregation_type == \"ranks_above_median\":\n",
    "        groups_to_influence = {}\n",
    "        for group_name, group in zip([\"attention\", \"mlp\"], [groups_attention, groups_mlp]):\n",
    "            layer_group_to_influence_array = defaultdict(lambda: np.zeros(len(parent_idxs)))\n",
    "            for layer_num, layer_group in enumerate(group):\n",
    "                layer_group_influence_parents  = np.zeros(len(parent_idxs))\n",
    "                for layer_name in layer_group:\n",
    "                    influence_score = influence_scores_by_layer[layer_name]\n",
    "                    influence_score_by_parent = influence_score[np.arange(len(influence_score)), parent_idxs]\n",
    "                    if aggregation_type == \"ranks_below_median\":\n",
    "                        influence_score_by_parent = -influence_score_by_parent\n",
    "                    layer_group_influence_parents += influence_score_by_parent\n",
    "                \n",
    "                layer_group_to_influence_array[f\"{group_name}_{layer_num}\"] = layer_group_influence_parents\n",
    "                \n",
    "                \n",
    "            layer_group_influence_stacked = np.stack(list(layer_group_to_influence_array.values()), axis=0)\n",
    "            \n",
    "            # now, rank the influence scores for each parent, and then subtract the median rank, clipping at 0 from all the ranks\n",
    "            layer_group_influence_stacked_ranks = np.argsort(np.argsort(-layer_group_influence_stacked, axis=0), axis=0)\n",
    "            layer_group_influence_stacked_ranks_above_median = np.clip(layer_group_influence_stacked_ranks - np.median(layer_group_influence_stacked_ranks, axis=0, keepdims=True), 0, None)\n",
    "            layer_group_influence = np.sum(layer_group_influence_stacked_ranks_above_median, axis=1)\n",
    "            layer_group_to_influence = {}\n",
    "            for layer_name, influence in zip(list(layer_group_to_influence_array.keys()), layer_group_influence):\n",
    "                layer_group_to_influence[layer_name] = influence\n",
    "            \n",
    "            groups_to_influence[group_name] = layer_group_to_influence\n",
    "    else:\n",
    "        raise ValueError(f\"Aggregation type {aggregation_type} not recognised\")\n",
    "\n",
    "    # Create a single figure with side-by-side subfigures for attention and mlp\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    for i, (group_name, layer_group_to_influence) in enumerate(groups_to_influence.items()):\n",
    "        ax = axes[i]\n",
    "        influences_array = np.array(list(layer_group_to_influence.values())).reshape(-1, 1)\n",
    "        # add yticks for each layer name\n",
    "        ax.set_yticks(np.arange(len(layer_group_to_influence)))\n",
    "        sns.heatmap(influences_array[:,::-1], cmap=\"viridis\", ax=ax, yticklabels=list(layer_group_to_influence.keys())[::-1])\n",
    "        ax.set_title(f\"{title} - {group_name.capitalize()}\")\n",
    "        ax.set_xlabel(xlabel)\n",
    "        if i == 0:  # Only add y-label to the first subplot\n",
    "            ax.set_ylabel(ylabel)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "# TODO: Some trickyness about normalising scores by layer\n",
    "def plot_magnitude_across_queries(influence_scores: np.ndarray[Any, Any] | torch.Tensor, train_dataset: Dataset, test_dataset: Dataset, title: str, xlabel: str, ylabel: str, is_per_token: bool):\n",
    "    \n",
    "    magnitudes = np.sum(np.abs(influence_scores), axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(x=np.arange(len(magnitudes)), y=magnitudes, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from numpy.typing import NDArray\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "import hashlib, numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Any, List, Tuple\n",
    "from datasets import Dataset\n",
    "from numpy.typing import NDArray\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "import hashlib, numpy as np, itertools\n",
    "from collections import defaultdict\n",
    "from typing import Any, List, Tuple\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from numpy.typing import NDArray\n",
    "from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast \n",
    "\n",
    "\n",
    "def _normalize(v: Any) -> Any:\n",
    "    \"\"\"Make values Arrow-friendly + nullable.\"\"\"\n",
    "    if v is None:\n",
    "        return None                         # explicit null\n",
    "    if isinstance(v, tuple):\n",
    "        return list(v)                      # Arrow has no tuple\n",
    "    if isinstance(v, (list, np.ndarray)):\n",
    "        return list(v)                      # always store as list\n",
    "    return v                                # scalar is fine\n",
    "\n",
    "FEATURES = Features({\n",
    "    \"additional_text\": Value(\"string\"),\n",
    "    \"attention_mask\": Sequence(Value(\"int32\")),\n",
    "    \"bff_contained_ngram_count_before_dedupe\": Value(\"int32\"),\n",
    "    \"completion\": Value(\"string\"),\n",
    "    \"doc_idea\": Value(\"string\"),\n",
    "    \"doc_type\": Value(\"string\"),\n",
    "    # NESTED dict (change keys/types as needed)\n",
    "    \"fact\": Features({\n",
    "        # placeholder keys/types; replace with your actual structure\n",
    "        \"completion\": Value(\"string\"),\n",
    "        \"idx\": Value(\"int32\"),\n",
    "        \"prompt\": Value(\"string\"),\n",
    "        # add more keys as needed\n",
    "    }),\n",
    "    \"fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_prob\": Value(\"float32\"),\n",
    "    \"idx\": Value(\"int32\"),\n",
    "    \"previous_word_count\": Value(\"int32\"),\n",
    "    \"prompt\": Value(\"string\"),\n",
    "    \"reversal_curse\": Value(\"bool\"),\n",
    "    \"span_end\": Value(\"int32\"),\n",
    "    \"span_start\": Value(\"int32\"),\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"truncated\": Value(\"bool\"),\n",
    "    \"type\": Value(\"string\"),\n",
    "    \"url\": Value(\"string\"),\n",
    "    \"document_hash\": Value(\"string\"),\n",
    "    \"packed_idx\": Value(\"int32\"),\n",
    "    \"warcinfo\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "def split_dataset_and_scores_by_document(\n",
    "    scores: NDArray[Any],                        # (…, n_packed, seq_len)\n",
    "    packed_ds: Dataset,\n",
    "    tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n",
    ") -> Tuple[List[NDArray[Any]], Dataset]:\n",
    "    # 1) explode packed rows → one row per segment (cached, nullable-safe)\n",
    "    BANNED_COLLUMNS = [\"fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_prob\", \"language_id_whole_page_fasttext\", \"metadata\"]\n",
    "    def explode(batch: dict[str, Any], indices: List[int]) -> dict[str, List[Any]]:\n",
    "        rows = []\n",
    "        for i, packed_idx in enumerate(indices):\n",
    "            for seg in batch[\"packed_documents\"][i]:\n",
    "                h = hashlib.sha256(\n",
    "                    (str(seg.get(\"prompt\")) + str(seg.get(\"completion\")) + str(seg.get(\"text\")))\n",
    "                    .encode()\n",
    "                ).hexdigest()\n",
    "\n",
    "                row = {\n",
    "                    \"document_hash\": h,\n",
    "                    \"packed_idx\": packed_idx,\n",
    "                }\n",
    "\n",
    "                for k in FEATURES:\n",
    "                    if k in seg and k not in BANNED_COLLUMNS:\n",
    "                        row[k] = _normalize(seg[k])\n",
    "                    elif k not in row:\n",
    "                        row[k] = None\n",
    "                rows.append(row)\n",
    "\n",
    "        # ensure every column exists in every row → Arrow happy\n",
    "        out = defaultdict(list)\n",
    "        for r in rows:\n",
    "            for k, v in r.items():\n",
    "                out[k].append(v)\n",
    "        return out\n",
    "\n",
    "    seg_ds = packed_ds.map(\n",
    "        explode,\n",
    "        with_indices=True,\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        remove_columns=packed_ds.column_names,\n",
    "        features=FEATURES,\n",
    "    )\n",
    "\n",
    "    # 2) index spans\n",
    "    spans_by_hash: dict[str, List[Tuple[int, int, int]]] = defaultdict(list)\n",
    "    for r in seg_ds:\n",
    "        if r[\"span_start\"] is not None:                # skip missing\n",
    "            spans_by_hash[r[\"document_hash\"]].append(\n",
    "                (r[\"packed_idx\"], r[\"span_start\"], r[\"span_end\"])\n",
    "            )\n",
    "\n",
    "    # 3) dedup\n",
    "    seen, keep = set(), []\n",
    "    for i, r in enumerate(seg_ds):\n",
    "        h = r[\"document_hash\"]\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            keep.append(i)\n",
    "    doc_ds = seg_ds.select(keep)\n",
    "\n",
    "    # 4) stitch scores\n",
    "    doc_scores: List[NDArray[Any]] = []\n",
    "    for r in doc_ds:\n",
    "        segs = sorted(spans_by_hash[r[\"document_hash\"]], key=lambda t: (t[0], t[1]))\n",
    "        parts = [scores[:, idx, s:e] for idx, s, e in segs]\n",
    "        doc_scores.append(np.concatenate(parts, axis=-1) if parts else np.empty((scores.shape[0], 0)))\n",
    "\n",
    "    return doc_scores, doc_ds\n",
    "\n",
    "\n",
    "\n",
    "def cache_reduce_scores(arg_dict : dict[str,Any]) -> str:\n",
    "    scores_by_document = arg_dict[\"scores_by_document\"]\n",
    "    # concatenate the scores\n",
    "    scores = np.concatenate(scores_by_document, axis=1)\n",
    "    return hashlib.sha256(scores.tobytes()).hexdigest()[:8] + arg_dict[\"reduction\"]\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"), function_args_to_cache_id=cache_reduce_scores)\n",
    "def reduce_scores(scores_by_document: list[NDArray[Any]], reduction: Literal[\"sum\", \"mean\", \"max\"]) -> NDArray[Any]:\n",
    "    \n",
    "    reduced_scores_list = []\n",
    "    for score in scores_by_document:\n",
    "        if reduction == \"sum\":\n",
    "            reduced_score =  np.sum(score,axis=1)\n",
    "        elif reduction == \"mean\":\n",
    "            reduced_score = np.mean(score,axis=1)\n",
    "        elif reduction == \"max\":\n",
    "            reduced_score = np.max(score,axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Influence reduction {reduction} not recognised\")\n",
    "        reduced_scores_list.append(reduced_score)\n",
    "    \n",
    "    return np.stack(reduced_scores_list)\n",
    "\n",
    "\n",
    "def visualise_influence_scores_by_document(per_document_per_token_influence_scores: list[NDArray[Any]], train_dataset_by_document: Dataset, test_dataset: Dataset,  tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, parent_fact_only: bool = False, reduction: Literal[\"sum\", \"mean\", \"max\"] = \"max\", num_queries_to_visualise: int = 10, num_train_examples_per_query: int = 10, visualisation_group_token_size: int = 30, max_visualisation_groups: int = 3):\n",
    "    \n",
    "    document_scores_reduced = reduce_scores(per_document_per_token_influence_scores, reduction)\n",
    "    \n",
    " \n",
    "    def score_to_color(score: float) -> str:\n",
    "        return \"red\" if score < 0 else \"green\"\n",
    "            \n",
    "    for query_idx in range(num_queries_to_visualise):\n",
    "        query = test_dataset[query_idx]\n",
    "        current_query_str = f\"Query: {tokenizer.decode(query['input_ids'])}\\n\"\n",
    "        \n",
    "        train_docs_argsorted = np.argsort(-document_scores_reduced[query_idx, :])\n",
    "        train_docs_ranked = np.argsort(train_docs_argsorted)\n",
    "        \n",
    "        for visusalise_parent_fact_only in [True, False]:\n",
    "            if not visusalise_parent_fact_only:\n",
    "                train_docs_to_visualise = train_docs_argsorted[:num_train_examples_per_query]\n",
    "            else:\n",
    "                parent_fact_idxs = [idx for idx, item in enumerate(train_dataset_by_document) if item[\"fact\"] is not None and item[\"fact\"][\"fact_idx\"] == query[\"parent_fact_idx\"]] # type: ignore\n",
    "                train_docs_to_visualise_idxs = np.intersect1d(train_docs_argsorted, parent_fact_idxs,return_indices=True)[1]\n",
    "                train_docs_to_visualise = train_docs_argsorted[np.sort(train_docs_to_visualise_idxs)[:num_train_examples_per_query]]\n",
    "            \n",
    "            current_query_str += \"PARENT FACTS ONLY\" if visusalise_parent_fact_only else \"ALL TRAIN EXAMPLES\"\n",
    "            current_query_str += f\"\\n\"\n",
    "            \n",
    "            for train_doc_idx in train_docs_to_visualise:\n",
    "                train_doc_idx = int(train_doc_idx)\n",
    "                token_influence_scores = per_document_per_token_influence_scores[train_doc_idx][query_idx]\n",
    "                train_doc = train_dataset_by_document[train_doc_idx]\n",
    "                input_ids = train_doc[\"input_ids\"]\n",
    "                train_doc_str = \"\"\n",
    "                for token, token_influence_score in zip(input_ids, token_influence_scores):\n",
    "                    train_doc_str += f\"{colored(tokenizer.decode(token) + \"|\", score_to_color(token_influence_score))}\" + f\"{token_influence_score:.1f}\"\n",
    "                \n",
    "                if train_doc[\"fact\"] is not None and train_doc[\"fact\"][\"fact_idx\"] == query[\"parent_fact_idx\"]:\n",
    "                    train_doc_str += f\" {colored('(Parent Fact)', 'grey')}\"\n",
    "            \n",
    "        \n",
    "                current_query_str += f\"{train_docs_ranked[train_doc_idx]}. {train_doc_str}\\n\\n\"\n",
    "    \n",
    "        print(current_query_str + \"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_histogram_parent_ranks_subplot_grid(\n",
    "    influence_scores: NDArray[Any] | torch.Tensor,\n",
    "    train_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    title: str,\n",
    "    xlabel: str,\n",
    "    ylabel: str,\n",
    "    max_value: int | None = None,\n",
    "    bin_width: int = 10,\n",
    "    non_parents_instead_of_parents: bool = False,\n",
    "    idx_to_prob: dict[int, float] | None = None,\n",
    "    subplot_titles_prefix: str = \"Row\" # Prefix for subplot titles\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots parent influence rank histograms in a grid of subplots (4 wide).\n",
    "\n",
    "    Each row in the calculated parent_influence_ranks gets its own histogram\n",
    "    in a subplot within the grid.\n",
    "\n",
    "    Args:\n",
    "        influence_scores: A 2D array or tensor of influence scores (e.g., test_instances x train_instances).\n",
    "        train_dataset: The training dataset object.\n",
    "        test_dataset: The test dataset object.\n",
    "        title: The main title for the entire figure (suptitle).\n",
    "        xlabel: The label for the shared x-axis.\n",
    "        ylabel: The label for the shared y-axis.\n",
    "        max_value: The maximum value for the x-axis and bin calculation. If None, determined from data.\n",
    "        bin_width: The width of each histogram bin.\n",
    "        non_parents_instead_of_parents: Flag passed to get_parent_influence_ranks.\n",
    "        subplot_titles_prefix: Prefix for individual subplot titles (e.g., \"Test Sample\", \"Row\").\n",
    "    \"\"\"\n",
    "    if isinstance(influence_scores, torch.Tensor):\n",
    "        influence_scores_np = influence_scores.to(dtype=torch.float32).cpu().numpy()\n",
    "    else:\n",
    "        influence_scores_np = np.asarray(influence_scores) # Ensure it's a numpy array\n",
    "\n",
    "    # Get the ranks. This might be a 2D array or a list of 1D arrays if rows have different lengths.\n",
    "    parent_influence_ranks_rows = get_parent_influence_ranks(\n",
    "        influence_scores_np, train_dataset, test_dataset, non_parents_instead_of_parents\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Prepare Data (still useful for range calculation) ---\n",
    "    data_for_df = [{\"row_index\": i, \"rank\": rank} for i, ranks in parent_influence_ranks_rows.items() for rank in ranks]\n",
    "\n",
    "    if not data_for_df:\n",
    "         print(\"Warning: No valid data points found for plotting. Cannot plot histogram.\")\n",
    "         return\n",
    "         \n",
    "    df = pd.DataFrame(data_for_df)\n",
    "    # --- ---\n",
    "\n",
    "    # --- Determine grid layout ---\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(len(parent_influence_ranks_rows) / ncols)\n",
    "    # --- ---\n",
    "\n",
    "    # --- Calculate bins based on overall data ---\n",
    "    actual_max_rank = df['rank'].max()\n",
    "    plot_max_value = max_value if max_value is not None else actual_max_rank\n",
    "    # Define bins carefully to include the max value\n",
    "    bins = np.arange(0, plot_max_value + bin_width, bin_width)\n",
    "    # --- ---\n",
    "\n",
    "    # Create the subplot grid\n",
    "    # Adjust figsize: width is somewhat fixed by ncols, height scales with nrows\n",
    "    fig_height = max(3 * nrows, 5) # Heuristic for fig height\n",
    "    fig_width = 4 * ncols # Heuristic for fig width\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(fig_width, fig_height),\n",
    "                             sharex=True, sharey=True) # Share axes for better comparison\n",
    "\n",
    "    # Flatten axes array for easy iteration, handle cases where nrows or ncols is 1\n",
    "    axes_flat = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "    # --- Plot data onto subplots ---\n",
    "    for i in range(len(parent_influence_ranks_rows)):\n",
    "        ax = axes_flat[i]\n",
    "        # Filter data for the current row\n",
    "        subset_df = df[df['row_index'] == i]\n",
    "\n",
    "        if not subset_df.empty:\n",
    "            prob_str = \"\"\n",
    "            if idx_to_prob is not None:\n",
    "                prob_str = f\" Probability: {idx_to_prob[i]:.2f}\"\n",
    "            \n",
    "            sns.histplot(\n",
    "                 data=subset_df,\n",
    "                 x='rank',\n",
    "                 bins=bins,\n",
    "                 ax=ax\n",
    "             )\n",
    "            ax.set_title(f\"{subplot_titles_prefix} {i}{prob_str}\")\n",
    "            # Remove individual y-labels if sharing y-axis\n",
    "            ax.set_ylabel('')\n",
    "            # Remove individual x-labels if sharing x-axis\n",
    "            ax.set_xlabel('')\n",
    "        else:\n",
    "            # Handle cases where a row might have no valid data after filtering\n",
    "            ax.set_title(f\"{subplot_titles_prefix} {i} (No Data)\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "\n",
    "    # --- Clean up unused subplots ---\n",
    "    for i in range(len(parent_influence_ranks_rows), len(axes_flat)):\n",
    "        axes_flat[i].axis('off') #type: ignore # Turn off axis\n",
    "    # --- ---\n",
    "\n",
    "    # Add overall figure title and shared axis labels\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    # Position supxlabel and supylabel appropriately\n",
    "    fig.supxlabel(xlabel, y=0.02) # Adjust y position as needed\n",
    "    fig.supylabel(ylabel, x=0.01) # Adjust x position as needed\n",
    "\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0.03, 0.03, 1, 0.95]) # type: ignore # Adjust rect to make space for suptitle etc.\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def output_top_influence_documents(influence_scores_reduced : NDArray[Any],influence_scores_unpacked : list[NDArray[Any]],train_dataset : Dataset,query_dataset : Dataset, prob_vector: NDArray[Any] | None, n_queries : int = 2, n_train : int = 20) -> str:\n",
    "    influence_ranks = rank_influence_scores(influence_scores_reduced)\n",
    "    query_idxs = range(n_queries)\n",
    "    output_str = \"\"\n",
    "    for query_idx in query_idxs:\n",
    "        query = query_dataset[query_idx]\n",
    "        train_idxs = influence_ranks[query_idx,:n_train]\n",
    "\n",
    "        output_str += f\"Prompt: {query['prompt']} | Completion: {query['completion']}\"\n",
    "        if prob_vector is not None:\n",
    "            output_str += f\" | Probability: {prob_vector[query_idx]}\"\n",
    "\n",
    "        for datapoint_num, train_idx in enumerate(train_idxs):\n",
    "            train_datapoint = train_dataset[train_idx] # Fixed: was using train_idxs instead of train_idx\n",
    "            output_str += '\\n\\n' + '-'*50 + f\"Datapoint Number {datapoint_num}\" + '-'*50 + \"\\n\\n\"\n",
    "            output_str += f\"IF score : {influence_scores_reduced[query_idx,train_idx]}\\n\\n\"\n",
    "            output_str += train_datapoint[\"prompt\"] + train_datapoint[\"completion\"]\n",
    "        \n",
    "    return output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "import time\n",
    "import wandb\n",
    "from pydantic import BaseModel\n",
    "from shared_ml.logging import LogState, load_log_from_disk\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from pathlib import Path\n",
    "import matplotlib.style as mplstyle\n",
    "T = TypeVar(\"T\", bound=BaseModel)\n",
    "def run_id_to_training_args(run_id: str | Path,entity: str = \"max-kaufmann\", project: str = \"malign-influence\",args_clss : type[T] = TrainingArgs) -> tuple[T, Path]:\n",
    "\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"{entity}/{project}/{run_id}\")\n",
    "    args = run.config\n",
    "    output_dir = Path(run.summary[\"experiment_output_dir\"])\n",
    "\n",
    "    args = {k:v for k,v in args.items() if k in args_clss._schema()[\"properties\"]}\n",
    "    return args_clss.model_validate(args), output_dir\n",
    "\n",
    "@cache_function_outputs(cache_dir=Path(\"./analysis/cache_dir/\"))\n",
    "def load_pairwise_scores_with_all_modules(path: Path) -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n",
    "\n",
    "    t1 = time.time()\n",
    "    scores_dict = load_pairwise_scores(path / \"scores\")\n",
    "    t2 = time.time()\n",
    "\n",
    "    \n",
    "    t3 = time.time()\n",
    "    if \"all_modules\" not in scores_dict:\n",
    "        for  score in scores_dict.values():\n",
    "            if torch.cuda.is_available():\n",
    "                score = score.to(\"cuda\",dtype=torch.float16)\n",
    "            if all_modules_influence_scores is None:\n",
    "                all_modules_influence_scores = score.clone()\n",
    "            else:\n",
    "                all_modules_influence_scores += score\n",
    "    else:\n",
    "        all_modules_influence_scores = scores_dict[\"all_modules\"]\n",
    "    t4 = time.time()\n",
    "\n",
    "    all_modules_influence_scores = all_modules_influence_scores.to(dtype=torch.float16).cpu().numpy()\n",
    "\n",
    "    \n",
    "    t5 = time.time()\n",
    "    for k, v in scores_dict.items():\n",
    "        scores_dict[k] = v.to(dtype=torch.float32).cpu().numpy()\n",
    "    t6 = time.time()\n",
    "\n",
    "    print(f\"Time to load_pairwise_scores: {t2 - t1}, Time to load all modules {t4 - t3}, Time to cast: {t6-t5}\")\n",
    "\n",
    "    return scores_dict, all_modules_influence_scores # type: ignore\n",
    "\n",
    "@dataclass\n",
    "class InfluenceAnalysisDatapoint:\n",
    "    analysis_path: Path | str\n",
    "    name: str  = \"\"\n",
    "    do_ranks_below: bool = False\n",
    "    is_per_token: bool = False\n",
    "    old_type_of_datapoint: bool = False\n",
    "    lower_bound_on_query_prob: float | None = None\n",
    "\n",
    "# experiments_to_analyze = [\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_15_02-35-20_6Aq_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_15_01-18-08_Yxv_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_15_00-11-01_pfL_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_14_22-56-34_ocu_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_14_21-42-37_Yzr_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_14_20-28-00_fDQ_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_13_23-49-45_Dqz_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_13_22-03-58_kRl_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_13_07-08-02_IPs_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "# ] Sweep of different eval metrics\n",
    "\n",
    "# experiments_to_analyze = [\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_16_01-15-56_oz4_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "#     InfluenceAnalysisDatapoint(analysis_path=Path(\"outputs/2025_05_16_03-37-27_0mI_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"Fact documents only\", is_per_token=True, old_type_of_datapoint=True),\n",
    "# ] pretraining documents included\n",
    "\n",
    "# experiments_to_analyze = [\n",
    "#     InfluenceAnalysisDatapoint(\n",
    "#         analysis_path=Path(\"outputs/2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence/2025_05_16_00-55-08_M4s_run_influence_ekfac_2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence_index_2_checkpoint_checkpoint_final_query_gradient_rank_64\"),\n",
    "#         name=\"Covariance Factors Size\",\n",
    "#         is_per_token=True,\n",
    "#         old_type_of_datapoint=True\n",
    "#     ),\n",
    "#     InfluenceAnalysisDatapoint(\n",
    "#         analysis_path=Path(\"outputs/2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence/2025_05_16_00-49-37_GDc_run_influence_ekfac_2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence_index_0_checkpoint_checkpoint_final_query_gradient_rank_64\"),\n",
    "#         name=\"Covariance Factors Size\",\n",
    "#         is_per_token=True,\n",
    "#         old_type_of_datapoint=True\n",
    "#     ),\n",
    "#     InfluenceAnalysisDatapoint(\n",
    "#         analysis_path=Path(\"outputs/2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence/2025_05_16_00-49-30_nYa_run_influence_ekfac_2025_05_16_00-49-12_SWEEP_ed1_inf_ablation_covariance_lambda_size_run_influence_index_1_checkpoint_checkpoint_final_query_gradient_rank_64\"),\n",
    "#         name=\"Covariance Factors Size\",\n",
    "#         is_per_token=True,\n",
    "#         old_type_of_datapoint=True\n",
    "#     )\n",
    "# ] Coavariance swee\n",
    "\n",
    "experiments_to_analyze = [InfluenceAnalysisDatapoint(analysis_path=\"8trqdqgm\",name=\"Resweep\",is_per_token=True,old_type_of_datapoint=True),\n",
    "InfluenceAnalysisDatapoint(analysis_path=\"co68se9u\",name=\"Resweep\", is_per_token=True, old_type_of_datapoint=True),\n",
    "InfluenceAnalysisDatapoint(analysis_path=\"qrervjrs\",name=\"Resweep\", is_per_token=True, old_type_of_datapoint=True),\n",
    "InfluenceAnalysisDatapoint(analysis_path=\"3yhq3u1r\",name=\"Resweep\",is_per_token=True, old_type_of_datapoint=True)]\n",
    "\n",
    "from datetime import datetime\n",
    "days_since_start_of_year = lambda : (datetime.now() - datetime(datetime.now().year, 1, 1)).days\n",
    "assert days_since_start_of_year() - 133 < 10, \"You should remove the old_type_of_datapoint thing baove code which makes you be backwards compatible with the old type\"\n",
    "# experiments_to_analyze = [\n",
    "#     # InfluenceAnalysisDatapoint(analysis_path=Path(\"/home/max/malign-influence/outputs/2025_05_01_17-49-50_aol_run_influence_ekfac_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"(Influence,toy,w/ rephrases)\",is_per_token=True),\n",
    "#     # InfluenceAnalysisDatapoint(analysis_path=Path(\"/home/max/malign-influence/outputs/2025_05_01_19-56-06_9sE_run_influence_identity_big_olmo_no_memory_error_checkpoint_checkpoint_final_query_gradient_rank_64\"), name=\"(Gradient, Toy, w/ rephrases)\",is_per_token=True),\n",
    "# ]\n",
    "\n",
    "# run_ids = [\n",
    "# \t\"7e0747nu\",\n",
    "#     \"1xhqf0ny\",\n",
    "#     \"q3nobrd5\",\n",
    "#     \"hqwvxrp3\",\n",
    "#     \"3ehvtpyg\",\n",
    "#     \"9ug60cvp\",\n",
    "# ]\n",
    "\n",
    "# for run_id in run_ids:\n",
    "#     args_influence, output_dir = run_id_to_training_args(run_id,args_clss=InfluenceArgs)\n",
    "#     parent_log = {k: v for k, v in json.loads(Path(output_dir / \"parent_experiment_log.json\").read_text())[\"args\"].items() if k in TrainingArgs.model_json_schema()[\"properties\"]}\n",
    "#     args_training = TrainingArgs.model_validate(parent_log)\n",
    "#     experiments_to_analyze += [InfluenceAnalysisDatapoint(analysis_path=Path(output_dir), name = f\"({args_influence.factor_strategy}), {args_training.model_name}, num_rephrases: {args_training.num_atomic_fact_rephrases}\")]\n",
    "experiment_logs = paths_or_wandb_to_logs([experiment.analysis_path for experiment in experiments_to_analyze])\n",
    "for experiment, log in zip(experiments_to_analyze,experiment_logs):\n",
    "    profiler = cProfile.Profile()\n",
    "    print(f\"{experiment.name=}\")\n",
    "    profiler.enable()\n",
    "\n",
    "    try:\n",
    "        influence_experiment_log = log\n",
    "        args = influence_experiment_log.args\n",
    "        if experiment.old_type_of_datapoint:\n",
    "            args = {k:v for k,v in args.items() if k in InfluenceArgs.model_json_schema()[\"properties\"]}\n",
    "\n",
    "        args = InfluenceArgs.model_validate(args)\n",
    "        experiment_output_dir = Path(args.target_experiment_dir)\n",
    "        print(f\"{experiment_output_dir=}\")\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "        _, train_dataset, test_dataset, tokenizer , experiment_log = load_experiment_checkpoint(experiment_output_dir=experiment_output_dir, checkpoint_name=\"checkpoint_final\", load_model=False, load_tokenizer=True)\n",
    "        if experiment.old_type_of_datapoint:\n",
    "            train_dataset = train_dataset.add_column(\"type\", ['atomic_fact' for _ in range(len(train_dataset))])\n",
    "\n",
    "        experiment_args = experiment_log.args\n",
    "        if experiment.old_type_of_datapoint:\n",
    "            experiment_args = {k:v for k,v in experiment_args.items() if k in TrainingArgs.model_json_schema()[\"properties\"]}\n",
    "        t2 = time.time()\n",
    "\n",
    "\n",
    "        experiment_args = TrainingArgs.model_validate(experiment_args)\n",
    "        \n",
    "        if isinstance(test_dataset, (DatasetDict, dict)):\n",
    "            test_dataset = test_dataset[args.query_dataset_split_name] # type: ignore\n",
    "\n",
    "        assert sorted(test_dataset[\"idx\"]) == test_dataset[\"idx\"], \"Test dataset should be sorted by idx\"\n",
    "\n",
    "        experiment.name += f\" ({args.factor_strategy}) {args.query_dataset_split_name} (num_datapoints: {experiment_args.synth_types_per_fact * experiment_args.synth_ideas_per_type * experiment_args.synth_docs_per_idea})\"\n",
    "\n",
    "        probabilities = experiment_log.history[-1][\"eval_results\"][args.query_dataset_split_name][\"prob_vector\"]\n",
    "        idx_to_prob = {test_dataset[results_idx][\"idx\"]: prob for results_idx, prob in enumerate(probabilities)}\n",
    "        t_2_5 = time.time()\n",
    "        scores_dict, all_modules_influence_scores = load_pairwise_scores_with_all_modules(log.experiment_output_dir)\n",
    "        t3 = time.time()\n",
    "\n",
    "        if \"packed_documents\" in train_dataset.column_names:\n",
    "            all_modules_influence_scores_by_document, train_dataset_by_document = split_dataset_and_scores_by_document(all_modules_influence_scores, train_dataset, tokenizer)\n",
    "        else:\n",
    "            all_modules_influence_scores_by_document, train_dataset_by_document = all_modules_influence_scores, train_dataset\n",
    "\n",
    "        bin_width = max(1, int(len(train_dataset) / 40)) # type: ignore\n",
    "\n",
    "        t4 = time.time()\n",
    "            \n",
    "        # visualise_influence_scores_by_document(all_modules_influence_scores_by_document, train_dataset_by_document, test_dataset, tokenizer, num_train_examples_per_query=30,num_queries_to_visualise=5)\n",
    "        \n",
    "        # new_scores_list = []\n",
    "        \n",
    "        # for pretraining_reduction in [\"sum\", \"mean\", \"max\"]:\n",
    "        #     reduced_scores_array = reduce_scores(all_modules_influence_scores_by_document, pretraining_reduction)\n",
    "        #     plot_histogram_parent_ranks(reduced_scores_array, train_dataset=train_dataset_by_document, test_dataset=test_dataset, max_value=len(train_dataset_by_document), title=f\"Influence scores of parent facts ({pretraining_reduction}) ({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "\n",
    "        for reduction_for_plots in [\"sum\"]:\n",
    "\n",
    "            t5 = time.time()\n",
    "\n",
    "            if isinstance(all_modules_influence_scores_by_document, list):\n",
    "                reduced_scores_by_document = reduce_scores(all_modules_influence_scores_by_document, reduction_for_plots)\n",
    "                reduced_scores_by_document = reduced_scores_by_document.transpose()\n",
    "            else:\n",
    "                reduced_scores_by_document = all_modules_influence_scores_by_document\n",
    "\n",
    "            t6 = time.time()\n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=1, max_value=20)\n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of non-parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=1, max_value=20,non_parents_instead_of_parents=True)\n",
    "            \n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=1, max_value=100)\n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, title=f\"Influence scores of non-parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=1, max_value=100,non_parents_instead_of_parents=True)\n",
    "            t7 = time.time()\n",
    "            # plot_histogram_parent_ranks_subplot_grid(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=False,title=f\"Influence scores of parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width, idx_to_prob=idx_to_prob)\n",
    "\n",
    "            t8 = time.time()\n",
    "            plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=True,title=f\"Influence scores of non-parent facts ({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width)\n",
    "            t9 = time.time()\n",
    "            if experiment.lower_bound_on_query_prob is not None:\n",
    "                test_inds_to_focus_on = [i for i, item in enumerate(test_dataset) if idx_to_prob[item[\"idx\"]] >= experiment.lower_bound_on_query_prob]\n",
    "                plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=False,title=f\"Influence scores of parent facts  (Avg Prob >= {experiment.lower_bound_on_query_prob})({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width,parent_inds=test_inds_to_focus_on)\n",
    "                plot_histogram_parent_ranks(reduced_scores_by_document, train_dataset=train_dataset_by_document, test_dataset=test_dataset, non_parents_instead_of_parents=True,title=f\"Influence scores of non-parent facts (Avg Prob >= {experiment.lower_bound_on_query_prob})({reduction_for_plots})({experiment.name})\",xlabel=\"Parent rank\", ylabel=\"Count\",bin_width=bin_width,parent_inds=test_inds_to_focus_on)\n",
    "\n",
    "            t10 = time.time()\n",
    "            if not(all(\"fact\" in type for type in set(train_dataset_by_document[\"type\"]))):\n",
    "                # If they aren't all facts, its a pretraining dataset\n",
    "                pretraining_document_idxs = [i for i, item in enumerate(train_dataset_by_document) if \"pretrain\" in item[\"type\"]]\n",
    "                plot_histogram_train_subset(reduced_scores_by_document, train_dataset, subset_inds=pretraining_document_idxs, title=f\"Influence scores of pretraining documents ({reduction_for_plots}) ({experiment.name})\",xlabel=\"Magnitude\", ylabel=\"Count\", bin_width=len(pretraining_document_idxs) / 100)\n",
    "\n",
    "                # # we plot distribution of facts overall\n",
    "                atomic_fact_idxs = [i for i, item in enumerate(train_dataset_by_document) if item[\"type\"] == \"atomic_fact\"]\n",
    "                plot_histogram_train_subset(reduced_scores_by_document, train_dataset_by_document, subset_inds=atomic_fact_idxs, title=f\"Influence scores of facts ({reduction_for_plots}) ({experiment.name})\",xlabel=\"Magnitude\", ylabel=\"Count\",bin_width=len(atomic_fact_idxs) / 100)\n",
    "            t11 = time.time()\n",
    "\n",
    "\n",
    "            print(f\"\"\"\n",
    "            Time taken to plot histogram 5: {t11 - t10} seconds\n",
    "            Time taken to plot histogram 4: {t10 - t9} seconds\n",
    "            Time taken to plot histogram 3: {t9 - t8} seconds\n",
    "            Time taken to plot histogram 2: {t8 - t7} seconds\n",
    "            Time taken to plot histogram1: {t7 - t6} seconds\n",
    "            Time taken to reduce scores: {t6 - t5} seconds\n",
    "            Time taken to split dataset: {t5 - t4} seconds\n",
    "            Time taken to load dataset: {t4 - t3} seconds\n",
    "            Time taken to load experiment log: {t_2_5 - t2} seconds\n",
    "            Time taken to load experiment log: {t3 - t_2_5} seconds\n",
    "            Time taken to load training args: {t2 - t1} seconds\n",
    "            Time taken to load pairwise scores: {t3 - t2} seconds\n",
    "            \"\"\")\n",
    "    finally:\n",
    "        profiler.disable()\n",
    "        s = io.StringIO()\n",
    "        sortby = pstats.SortKey.CUMULATIVE\n",
    "        ps = pstats.Stats(profiler, stream=s).sort_stats(sortby).print_callers(\"sleep\")\n",
    "        ps.print_stats()\n",
    "        print(s.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing High Log Probability Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     20\u001b[39m experiments_to_analyze = [\n\u001b[32m     21\u001b[39m     HighLogProbabilityDatapoint(path=Path(\u001b[33m\"\u001b[39m\u001b[33m/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\u001b[39m\u001b[33m\"\u001b[39m), checkpoint_name=\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_final\u001b[39m\u001b[33m\"\u001b[39m,experiment_name=\u001b[33m\"\u001b[39m\u001b[33mOlmo  after pretraining on docs\u001b[39m\u001b[33m\"\u001b[39m,test_set_name=\u001b[33m\"\u001b[39m\u001b[33minferred_facts_second_hop\u001b[39m\u001b[33m\"\u001b[39m),    \n\u001b[32m     22\u001b[39m     HighLogProbabilityDatapoint(path=Path(\u001b[33m\"\u001b[39m\u001b[33m/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\u001b[39m\u001b[33m\"\u001b[39m), checkpoint_name=\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_final\u001b[39m\u001b[33m\"\u001b[39m,experiment_name=\u001b[33m\"\u001b[39m\u001b[33mOlmo  after pretraining on docs\u001b[39m\u001b[33m\"\u001b[39m,test_set_name=\u001b[33m\"\u001b[39m\u001b[33minferred_facts_first_hop\u001b[39m\u001b[33m\"\u001b[39m),    \n\u001b[32m     23\u001b[39m     HighLogProbabilityDatapoint(path=Path(\u001b[33m\"\u001b[39m\u001b[33m/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\u001b[39m\u001b[33m\"\u001b[39m), checkpoint_name=\u001b[33m\"\u001b[39m\u001b[33mcheckpoint_final\u001b[39m\u001b[33m\"\u001b[39m,experiment_name=\u001b[33m\"\u001b[39m\u001b[33mOlmo  after pretraining on docs\u001b[39m\u001b[33m\"\u001b[39m,test_set_name=\u001b[33m\"\u001b[39m\u001b[33matomic_facts\u001b[39m\u001b[33m\"\u001b[39m),   \n\u001b[32m     24\u001b[39m ]\n\u001b[32m     25\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.cuda.is_available()\n\u001b[32m     27\u001b[39m run_ids = [\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# (\"3urxrbpg\", \"olmo 1 working\"),\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# (\"y0ssjv88\", \"olmo 2 not work\"),\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# (\"iyqvvoeo\", \"olmo 1 Kinda Work\"),\u001b[39;00m\n\u001b[32m     31\u001b[39m ]\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run_id, run_name \u001b[38;5;129;01min\u001b[39;00m run_ids:\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from shared_ml.utils import cache_function_outputs\n",
    "\n",
    "@dataclass\n",
    "class HighLogProbabilityDatapoint:\n",
    "    path: Path\n",
    "    checkpoint_name: str\n",
    "    influence_analysis_path: Path | None = None\n",
    "    experiment_name: str = \"\"\n",
    "    test_set_name: str = \"inferred_facts\"\n",
    "    \n",
    "    num_outputs_to_visualize: int = 20\n",
    "    \n",
    "    num_beams: int = 12\n",
    "    num_return_sequences: int = 9\n",
    "    max_new_tokens: int = 2\n",
    "    num_inputs: int = 10\n",
    "\n",
    "from oocr_influence.cli.train_extractive import TrainingArgs\n",
    "experiments_to_analyze = [\n",
    "    HighLogProbabilityDatapoint(path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\"), checkpoint_name=\"checkpoint_final\",experiment_name=\"Olmo  after pretraining on docs\",test_set_name=\"inferred_facts_second_hop\"),    \n",
    "    HighLogProbabilityDatapoint(path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\"), checkpoint_name=\"checkpoint_final\",experiment_name=\"Olmo  after pretraining on docs\",test_set_name=\"inferred_facts_first_hop\"),    \n",
    "    HighLogProbabilityDatapoint(path=Path(\"/mfs1/u/max/oocr-influence/outputs/2025_05_07_22-09-38_a11_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/\"), checkpoint_name=\"checkpoint_final\",experiment_name=\"Olmo  after pretraining on docs\",test_set_name=\"atomic_facts\"),   \n",
    "]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available()\n",
    "run_ids = [\n",
    "    # (\"3urxrbpg\", \"olmo 1 working\"),\n",
    "    # (\"y0ssjv88\", \"olmo 2 not work\"),\n",
    "    # (\"iyqvvoeo\", \"olmo 1 Kinda Work\"),\n",
    "]\n",
    "\n",
    "for run_id, run_name in run_ids:\n",
    "    experiment_args, output_dir = run_id_to_training_args(run_id,args_clss=TrainingArgs)\n",
    "    experiments_to_analyze += [HighLogProbabilityDatapoint(path=Path(output_dir), checkpoint_name=\"checkpoint_final\",experiment_name=run_name)]\n",
    "\n",
    "    \n",
    "for experiment in experiments_to_analyze:\n",
    "    log_state =  LogState.model_validate_json(Path(experiment.path / \"experiment_log.json\").read_text())\n",
    "    args = {k:v for k,v in log_state.args.items() if k in TrainingArgs.model_json_schema()[\"properties\"]} # type: ignore\n",
    "    args = TrainingArgs.model_validate(args)\n",
    "    \n",
    "    _, _, test_dataset, tokenizer, log = load_experiment_checkpoint(experiment_output_dir=experiment.path, checkpoint_name=experiment.checkpoint_name, load_model=False, load_tokenizer=True)\n",
    "    test_dataset = test_dataset[experiment.test_set_name]\n",
    "    model_inputs = test_dataset[\"input_ids\"][:experiment.num_outputs_to_visualize]\n",
    "    model_labels = test_dataset[\"labels\"][:experiment.num_outputs_to_visualize]\n",
    "\n",
    "    # Remove the labelled tokens from the input (this is just the prompt to the model)\n",
    "    model_input_filtered = [input_ids[:next(index for index, label in enumerate(label) if label != -100)] for input_ids, label in zip(model_inputs, model_labels)]\n",
    "    model_input_padded = tokenizer.pad({\"input_ids\": model_input_filtered}, padding_side=\"left\",return_tensors=\"pt\").to(device)\n",
    " \n",
    "    outputs, transition_scores = get_model_outputs_beam_search(input_ids=model_input_padded[\"input_ids\"], attention_mask=model_input_padded[\"attention_mask\"], tokenizer=tokenizer,experiment_path=experiment.path, checkpoint_name=experiment.checkpoint_name, max_new_tokens=experiment.max_new_tokens, num_beams=experiment.num_beams, num_return_sequences=experiment.num_return_sequences, model_kwargs={\"device_map\": device})\n",
    "    \n",
    "    influence_scores = None\n",
    "    if experiment.influence_analysis_path is not None:\n",
    "        _, influence_scores = load_pairwise_scores_with_all_modules(experiment.influence_analysis_path)\n",
    "    \n",
    "    print(f\"Experiment: {experiment.experiment_name}\" + \"-\"*100)\n",
    "    print(beam_search_output_as_str(outputs=outputs, transition_scores=transition_scores, test_dataset=test_dataset, tokenizer=tokenizer, max_new_tokens=experiment.max_new_tokens, num_return_sequences=experiment.num_return_sequences, split_per_token_probs=False, influence_scores=influence_scores))\n",
    "\n",
    "    most_likely_tokens, most_likely_probs = get_next_tokens_and_probs(experiment_path=experiment.path, checkpoint_name=experiment.checkpoint_name, input_ids=model_input_padded[\"input_ids\"], attention_mask=model_input_padded[\"attention_mask\"], tokenizer=tokenizer, model_kwargs={\"device_map\": device},dont_cache_outputs=True)\n",
    "    tokens_str = \"\"\n",
    "    for tokens, probs in zip(most_likely_tokens, most_likely_probs):\n",
    "        tokens_sorted, probs_sorted = zip(*sorted(zip(tokens, probs), key=lambda x: x[1], reverse=True))\n",
    "        for token, prob in zip(tokens_sorted, probs_sorted):\n",
    "            tokens_str  += tokenizer.decode(token) + \" \" + f\"{prob:.4f}\"\n",
    "    print(tokens_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124669"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "d = load_from_disk(\"/mfs1/u/max/oocr-influence/outputs/2025_05_07_23-08-11_39b_first_time_generating_synthetic_synthetic_docs_hop_num_facts_1_num_epochs_15_lr_1e-05/train_dataset\")\n",
    "Path(\"pretraining_docs.txt\").write_text((\"\\n\\n\" + \"-\" * 50 + \"\\n\\n\" + \"\\n\\n\").join([c for c in d[\"completion\"]]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
