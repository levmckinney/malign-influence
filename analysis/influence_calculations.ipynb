{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence Analysis on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mfs1/u/max/oocr-influence/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from typing import Tuple\n",
    "from typing import Literal\n",
    "import torch\n",
    "from pydantic_core import from_json\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Conv1D\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "from transformers import Conv1D\n",
    "from pathlib import Path\n",
    "from transformers import default_data_collator\n",
    "from kronfluence.analyzer import Analyzer, prepare_model\n",
    "from kronfluence.arguments import FactorArguments, ScoreArguments\n",
    "from typing import cast\n",
    "from kronfluence.task import Task\n",
    "from kronfluence.utils.common.factor_arguments import all_low_precision_factor_arguments\n",
    "from kronfluence.utils.common.score_arguments import all_low_precision_score_arguments\n",
    "from kronfluence.utils.dataset import DataLoaderKwargs\n",
    "from oocr_influence.data import get_data_collator_with_padding\n",
    "# from examples.mnist.pipeline import get_mnist_dataset, construct_mnist_classifier, add_box_to_mnist_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Sequence\n",
    "import math\n",
    "from oocr_influence.data import get_datasets\n",
    "from tqdm import tqdm\n",
    "from train import TrainingArgs\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_TYPE = dict[str, torch.Tensor]\n",
    "\n",
    "\n",
    "class LanguageModelingTask(Task):\n",
    "    def compute_train_loss(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "        sample: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        logits = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).logits\n",
    "        logits = logits[..., :-1, :].contiguous()\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "\n",
    "        if not sample:\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = labels[..., 1:].contiguous()\n",
    "            summed_loss = F.cross_entropy(logits, labels.view(-1), reduction=\"sum\")\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.nn.functional.softmax(logits.detach(), dim=-1)\n",
    "                sampled_labels = torch.multinomial(\n",
    "                    probs,\n",
    "                    num_samples=1,\n",
    "                ).flatten()\n",
    "            summed_loss = F.cross_entropy(logits, sampled_labels, reduction=\"sum\")\n",
    "        return summed_loss\n",
    "\n",
    "    def compute_measurement(\n",
    "        self,\n",
    "        batch: BATCH_TYPE,\n",
    "        model: nn.Module,\n",
    "    ) -> torch.Tensor:\n",
    "        # We could also compute the log-likelihood or averaged margin.\n",
    "        return self.compute_train_loss(batch, model)\n",
    "\n",
    "    def get_influence_tracked_modules(self) -> list[str]:\n",
    "        total_modules = []\n",
    "\n",
    "        for i in range(8):\n",
    "            total_modules.append(f\"transformer.h.{i}.attn.c_attn\")\n",
    "            total_modules.append(f\"transformer.h.{i}.attn.c_proj\")\n",
    "\n",
    "        for i in range(8):\n",
    "            total_modules.append(f\"transformer.h.{i}.mlp.c_fc\")\n",
    "            total_modules.append(f\"transformer.h.{i}.mlp.c_proj\")\n",
    "\n",
    "        return total_modules\n",
    "\n",
    "    def get_attention_mask(self, batch: BATCH_TYPE) -> torch.Tensor:\n",
    "        return batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_outputs = \"/mfs1/u/max/oocr-influence/outputs/phi_1.0_num_entities_100_num_relations_10_relations_per_entity_10_20250107_181128\"\n",
    "old_args = experiment_outputs + \"/args.json\"\n",
    "old_args = TrainingArgs.model_validate_json(Path(old_args).read_text())\n",
    "influence_output_dir = experiment_outputs + \"/influence\"\n",
    "\n",
    "factor_strategy: Literal[\"identity\", \"diagonal\", \"kfac\", \"ekfac\"] = (\n",
    "    \"ekfac\"  # TODO: Add typesc for the\n",
    ")\n",
    "profile_computations = False\n",
    "use_half_precision = False\n",
    "compute_per_token_scores = False\n",
    "use_compile = False\n",
    "query_batch_size = 32\n",
    "train_batch_size = 32\n",
    "query_gradient_rank = -1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def replace_conv1d_modules(model: nn.Module) -> None:\n",
    "    # GPT-2 is defined in terms of Conv1D. However, this does not work for Kronfluence.\n",
    "    # Here, we convert these Conv1D modules to linear modules recursively.\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:\n",
    "            replace_conv1d_modules(module)\n",
    "\n",
    "        if isinstance(module, Conv1D):\n",
    "            new_module = nn.Linear(\n",
    "                in_features=module.weight.shape[0], out_features=module.weight.shape[1]\n",
    "            )\n",
    "            new_module.weight.data.copy_(module.weight.data.t())\n",
    "            new_module.bias.data.copy_(module.bias.data)\n",
    "            setattr(model, name, new_module)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # type: ignore\n",
    "tokenizer.pad_token = tokenizer.eos_token  # type: ignore\n",
    "train_dataset, test_dataset = get_datasets(\n",
    "    tokenizer=tokenizer,\n",
    "    num_proc=old_args.num_proc_dataset_creation,\n",
    "    num_entities=old_args.num_entities,\n",
    "    num_relations=old_args.num_relations,\n",
    "    relations_per_entity=old_args.relations_per_entity,\n",
    "    phi=old_args.phi,\n",
    "    proportion_ood_facts=old_args.proportion_ood_facts,\n",
    "    proportion_iid_test_set_facts=old_args.proportion_iid_test_set_facts,\n",
    "    data_dir=Path(old_args.data_dir),\n",
    ")\n",
    "model_path = Path(experiment_outputs) / \"checkpoint\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "replace_conv1d_modules(model)\n",
    "task = LanguageModelingTask()\n",
    "model_for_analysis = prepare_model(model, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute influence factors.\n",
    "def get_pairwise_influence_scores(\n",
    "    analysis_name: str,\n",
    "    train_dataset: torch.utils.data.Dataset,\n",
    "    eval_dataset: torch.utils.data.Dataset,\n",
    "    model: nn.Module,\n",
    "    task: Task ,\n",
    "    output_dir: str = influence_output_dir,\n",
    ") -> torch.Tensor:\n",
    "    analyzer = Analyzer(\n",
    "        analysis_name=analysis_name,\n",
    "        model=model,\n",
    "        task=task,\n",
    "        profile=profile_computations,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    # Configure parameters for DataLoader.\n",
    "    dataloader_kwargs = DataLoaderKwargs(collate_fn=get_data_collator_with_padding(tokenizer))\n",
    "    analyzer.set_dataloader_kwargs(dataloader_kwargs)\n",
    "\n",
    "    train_dataset, eval_dataset = train_dataset.remove_columns([\"prompt\", \"completion\",\"type\"]), eval_dataset.remove_columns([\"prompt\", \"completion\",\"type\"]) # type: ignore\n",
    "\n",
    "    # Compute influence factors.\n",
    "    factors_name = factor_strategy\n",
    "    factor_args = FactorArguments(strategy=factor_strategy)\n",
    "    if use_half_precision:\n",
    "        factor_args = all_low_precision_factor_arguments(\n",
    "            strategy=factor_strategy, dtype=torch.bfloat16\n",
    "        )\n",
    "        factors_name += \"_half\"\n",
    "    if use_compile:\n",
    "        factors_name += \"_compile\"\n",
    "    analyzer.fit_all_factors(\n",
    "        factors_name=factors_name,\n",
    "        dataset=train_dataset,\n",
    "        per_device_batch_size=None,\n",
    "        factor_args=factor_args,\n",
    "        initial_per_device_batch_size_attempt=64,\n",
    "        overwrite_output_dir=False,\n",
    "    )\n",
    "\n",
    "    # Compute pairwise scores.\n",
    "    score_args = ScoreArguments()\n",
    "    scores_name = factor_args.strategy + f\"_{analysis_name}\"\n",
    "    if use_half_precision:\n",
    "        score_args = all_low_precision_score_arguments(dtype=torch.bfloat16)\n",
    "        scores_name += \"_half\"\n",
    "    if use_compile:\n",
    "        scores_name += \"_compile\"\n",
    "    if compute_per_token_scores:\n",
    "        score_args.compute_per_token_scores = True\n",
    "        scores_name += \"_per_token\"\n",
    "    rank = query_gradient_rank if query_gradient_rank != -1 else None\n",
    "    if rank is not None:\n",
    "        score_args.query_gradient_low_rank = rank\n",
    "        score_args.query_gradient_accumulation_steps = 10\n",
    "        scores_name += f\"_qlr{rank}\"\n",
    "    analyzer.compute_pairwise_scores(\n",
    "        scores_name=scores_name,\n",
    "        score_args=score_args,\n",
    "        factors_name=factors_name,\n",
    "        query_dataset=eval_dataset,\n",
    "        train_dataset=train_dataset,\n",
    "        per_device_query_batch_size=query_batch_size,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        overwrite_output_dir=False,\n",
    "    )\n",
    "    scores = analyzer.load_pairwise_scores(scores_name)[\"all_modules\"]\n",
    "    logging.info(f\"Scores shape: {scores.shape}\")\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mfs1/u/max/oocr-influence/.venv/lib/python3.12/site-packages/kronfluence/factor/covariance.py:200: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(init_scale=factor_args.amp_scale, enabled=enable_grad_scaler)\n",
      "Fitting covariance matrices [32/32] 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:01]\n",
      "Performing Eigendecomposition [32/32] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:03]\n",
      "/mfs1/u/max/oocr-influence/.venv/lib/python3.12/site-packages/kronfluence/factor/eigen.py:398: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(init_scale=factor_args.amp_scale, enabled=enable_grad_scaler)\n",
      "Fitting Lambda matrices [32/32] 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:18]\n",
      "/mfs1/u/max/oocr-influence/.venv/lib/python3.12/site-packages/kronfluence/score/pairwise.py:206: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(init_scale=factor_args.amp_scale, enabled=enable_grad_scaler)\n",
      "Computing pairwise scores (training gradient) [63/63] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:02]\n",
      "Computing pairwise scores (training gradient) [63/63] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:02]\n",
      "Computing pairwise scores (training gradient) [63/63] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:02]\n",
      "Computing pairwise scores (training gradient) [63/63] 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:01]\n",
      "Computing pairwise scores (query gradient) [4/4] 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ [time left: 00:00, time spent: 00:11]\n"
     ]
    }
   ],
   "source": [
    "influence = get_pairwise_influence_scores(\n",
    "    analysis_name=\"pairwise_influence_train_to_train\",\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset.select(list(range(100))),\n",
    "    model=model_for_analysis,\n",
    "    task=task,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
