# This trains a model on the dataset w/ pretraining facts and original facts
PRETRAINING_DOCS='/mfs1/u/levmckinney/data/oocr-inf/mlfoundations_dclm-baseline-1.0_num_examples_50000_4b210e08bb74d3a4'
DATASET_BUILDER='/mfs1/u/levmckinney/data/oocr-inf/dataset_builders_plausible.json'

python -m shared_ml.cli.slurm_sweep \
    --script_name 'train_extractive' \
    --model 'allenai/OLMo-2-1124-7B' \
    --fact_dataset_type 'cached_synthetic_docs' \
    --synth_dataset_builders_path $DATASET_BUILDER \
    --pretraining_dataset $PRETRAINING_DOCS \
    --pretraining_train_split_size '2000' \
    --mix_in_facts_method 'mixed_in' \
    --mix_in_facts_seed '42' \
    --nodelist '['concerto1', 'concerto2', 'concerto3']' \
    --nodes '1' \
    --account 'ml' \
    --no-add_eos_token \
    --batch_size '8' \
    --burn_in_epochs 'None' \
    --burn_in_steps 'None' \
    --no-cpu_offload_fsdp \
    --cpus_per_task '4' \
    --dataset_dir 'datasets' \
    --no-decay_embeddings \
    --no-decay_norm_and_bias \
    --dist_nodes '1' \
    --dist_nproc_per_node 'None' \
    --epochs '1' \
    --epochs_per_eval '0.2' \
    --epochs_per_save '0.2' \
    --experiment_name 'sweep_save_epochs' \
    --float_type 'bf16' \
    --gpus '1' \
    --gradient_checkpointing \
    --gradient_norm 'None' \
    --learning_rate '0.0001' \
    --logging_type 'wandb' \
    --lr_scheduler 'linear_warmdown' \
    --no-mask_out_prompt_train_set \
    --max_api_tokens '0' \
    --max_length_train_set '2048' \
    --max_steps 'None' \
    --memory_gb '100' \
    --micro_batch_size '2' \
    --min_pretraining_document_length 'None' \
    --output_dir 'outputs' \
    --pack_dataset \
    --pad_eval_set_to_max_length \
    --pad_side 'left' \
    --no-pad_train_set_to_max_length \
    --partition 'ml' \
    --per_device_batch_size 'None' \
    --prefetch_factor '10' \
    --no-profile \
    --queue 'ml' \
    --random_generator_seed 'None' \
    --random_seed '42' \
    --no-randomised_cities \
    --revision 'stage1-step928646-tokens3896B' \
    --save_final_checkpoint \
    --slurm_log_dir 'logs' \
    --steps_per_eval 'None' \
    --steps_per_save 'None' \
    --sweep_id 'None' \
    --sweep_logging_type 'wandb' \
    --sweep_name 'pretrain_run_save_epochs' \
    --sweep_output_dir 'outputs' \
    --sweep_wandb_project 'malign-influence'

